#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„å¢å¼ºIEMOCAPè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«è®­ç»ƒç³»ç»Ÿ
ä¸“æ³¨äºGRUæ¶æ„æ”¹è¿›ï¼Œä¸ä½¿ç”¨å¤æ‚çš„è¯´è¯äººæ— å…³åˆ’åˆ†
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pickle
import argparse
from datetime import datetime
import warnings
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from models.enhanced_gru import create_enhanced_model
from utils import *

warnings.filterwarnings('ignore')

class SimpleEnhancedTrainer:
    """ç®€åŒ–çš„å¢å¼ºè®­ç»ƒå™¨"""
    
    def __init__(self, args):
        """åˆå§‹åŒ–è®­ç»ƒå™¨"""
        self.args = args
        self.device = torch.device('cuda' if torch.cuda.is_available() and args.cuda else 'cpu')
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(args.seed)
        
        # æƒ…æ„Ÿæ ‡ç­¾
        self.emotion_labels = {0: 'Angry', 1: 'Happy', 2: 'Neutral', 3: 'Sad'}
        self.emotion_colors = ['#e74c3c', '#f1c40f', '#95a5a6', '#3498db']
        
        # åˆ›å»ºå®éªŒç›®å½•
        self.exp_name = f"simple_enhanced_emotion_recognition_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.exp_dir = os.path.join('experiments', self.exp_name)
        os.makedirs(self.exp_dir, exist_ok=True)
        
        print(f"ğŸš€ ç®€åŒ–å¢å¼ºè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ å®éªŒç›®å½•: {self.exp_dir}")
        print(f"ğŸ–¥ï¸  è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ•°æ®
        self.load_data()
    
    def load_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ® - ä»¿ç…§train.pyçš„é€»è¾‘"""
        print("ğŸ“ åŠ è½½IEMOCAPæ•°æ®...")
        
        with open(self.args.data_path, 'rb') as f:
            data = pickle.load(f)
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(data)} ä¸ªä¼šè¯")
        
        # å±•å¼€æ‰€æœ‰ä¼šè¯çš„æ•°æ®
        all_samples = []
        for session_idx, session_data in enumerate(data):
            print(f"  ä¼šè¯ {session_idx}: {len(session_data)} ä¸ªæ ·æœ¬")
            all_samples.extend(session_data)
        
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(all_samples)}")
        
        # æå–ç‰¹å¾å’Œæ ‡ç­¾ - ä»¿ç…§utils.pyä¸­çš„Featureå‡½æ•°
        features = []
        labels = []
        valid_samples = 0
        
        for i, sample in enumerate(all_samples):
            try:
                # æŒ‰ç…§åŸå§‹æ•°æ®æ ¼å¼å¤„ç†
                if isinstance(sample, dict) and 'wav_encodings' in sample and 'label' in sample:
                    feature = sample['wav_encodings']
                    label = sample['label']
                    
                    # è½¬æ¢ä¸ºtensor
                    if not isinstance(feature, torch.Tensor):
                        feature = torch.tensor(feature, dtype=torch.float32)
                    
                    # ç¡®ä¿ç‰¹å¾ç»´åº¦æ­£ç¡® [seq_len, feature_dim]
                    if feature.dim() == 1:
                        feature = feature.unsqueeze(-1)  # [seq_len] -> [seq_len, 1]
                    elif feature.dim() > 2:
                        feature = feature.view(-1, feature.size(-1))  # flatten to [seq_len, feature_dim]
                    
                    # ç¡®ä¿æ ‡ç­¾æœ‰æ•ˆ
                    if isinstance(label, (int, float)) and 0 <= label <= 3:
                        features.append(feature)
                        labels.append(int(label))
                        valid_samples += 1
                    else:
                        print(f"âš ï¸ è·³è¿‡æ— æ•ˆæ ‡ç­¾æ ·æœ¬ {i}: label={label}")
                else:
                    print(f"âš ï¸ è·³è¿‡æ ¼å¼é”™è¯¯æ ·æœ¬ {i}: {type(sample)}")
                    
            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ ·æœ¬ {i} æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"ğŸ“Š æœ‰æ•ˆæ ·æœ¬æ•°: {valid_samples}")
        
        if valid_samples == 0:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬ï¼")
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„ç”¨äºåˆ’åˆ†
        self.features = features
        self.labels = labels
        
        # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
        label_counts = np.bincount(labels, minlength=4)
        print("ğŸ“ˆ æƒ…æ„Ÿåˆ†å¸ƒ:")
        for i, count in enumerate(label_counts):
            print(f"  {self.emotion_labels[i]}: {count} æ ·æœ¬")
    
    def create_data_loaders(self):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        print("ğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        
        # ç®€å•çš„è®­ç»ƒ/æµ‹è¯•åˆ’åˆ†
        train_features, test_features, train_labels, test_labels = train_test_split(
            self.features, self.labels, test_size=0.3, random_state=self.args.seed, 
            stratify=self.labels if len(set(self.labels)) > 1 else None
        )
        
        # ä»è®­ç»ƒé›†ä¸­å†åˆ’åˆ†å‡ºéªŒè¯é›†
        if len(train_features) > 2:
            train_features, val_features, train_labels, val_labels = train_test_split(
                train_features, train_labels, test_size=0.2, random_state=self.args.seed,
                stratify=train_labels if len(set(train_labels)) > 1 else None
            )
        else:
            val_features, val_labels = test_features, test_labels
        
        # å¡«å……åºåˆ—åˆ°ç›¸åŒé•¿åº¦
        def pad_sequences(features):
            if len(features) == 0:
                return torch.empty(0, 0, 0)
            
            max_len = max(f.size(0) for f in features)
            feature_dim = features[0].size(1) if features[0].dim() > 1 else features[0].size(0)
            
            padded = []
            for feat in features:
                if feat.dim() == 1:
                    feat = feat.unsqueeze(0)
                
                if feat.size(0) < max_len:
                    pad_len = max_len - feat.size(0)
                    feat = F.pad(feat, (0, 0, 0, pad_len))
                
                padded.append(feat)
            
            return torch.stack(padded)
        
        # å¡«å……ç‰¹å¾
        train_features_padded = pad_sequences(train_features)
        val_features_padded = pad_sequences(val_features)
        test_features_padded = pad_sequences(test_features)
        
        # è½¬æ¢æ ‡ç­¾ä¸ºtensor
        train_labels = torch.tensor(train_labels, dtype=torch.long)
        val_labels = torch.tensor(val_labels, dtype=torch.long)
        test_labels = torch.tensor(test_labels, dtype=torch.long)
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = TensorDataset(train_features_padded, train_labels)
        val_dataset = TensorDataset(val_features_padded, val_labels)
        test_dataset = TensorDataset(test_features_padded, test_labels)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=self.args.batch_size, shuffle=True, drop_last=False)
        val_loader = DataLoader(val_dataset, batch_size=self.args.batch_size, shuffle=False, drop_last=False)
        test_loader = DataLoader(test_dataset, batch_size=self.args.batch_size, shuffle=False, drop_last=False)
        
        print(f"ğŸ“Š æ•°æ®åˆ’åˆ†:")
        print(f"  è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        print(f"  éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
        print(f"  æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
        
        return train_loader, val_loader, test_loader
    
    def train_model(self):
        """è®­ç»ƒæ¨¡å‹"""
        print("\nğŸš€ å¼€å§‹è®­ç»ƒå¢å¼ºGRUæ¨¡å‹...")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader, test_loader = self.create_data_loaders()
        
        # åˆ›å»ºæ¨¡å‹ - åªä½¿ç”¨GRUéƒ¨åˆ†ï¼Œä¸ä½¿ç”¨HuBERT
        from models.enhanced_gru import EnhancedGRUModel
        
        # è·å–ç‰¹å¾ç»´åº¦
        sample_feature = self.features[0]
        input_dim = sample_feature.size(-1)
        
        model = EnhancedGRUModel(
            input_size=input_dim,
            hidden_size=self.args.hidden_layer,
            output_size=self.args.out_class,
            args=self.args
        )
        model = model.to(self.device)
        
        print(f"ğŸ¯ è¾“å…¥ç‰¹å¾ç»´åº¦: {input_dim}")
        print(f"ğŸ¯ æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # å®šä¹‰ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer = optim.AdamW(model.parameters(), lr=self.args.lr, weight_decay=self.args.weight_decay)
        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=self.args.T_0)
        
        # å®šä¹‰æŸå¤±å‡½æ•°
        criterion = nn.CrossEntropyLoss()
        
        # è®­ç»ƒå†å²
        train_history = {'loss': [], 'acc': []}
        val_history = {'loss': [], 'acc': []}
        
        best_val_acc = 0.0
        best_model_state = None
        patience_counter = 0
        
        print("ğŸ”„ å¼€å§‹è®­ç»ƒå¾ªç¯...")
        
        for epoch in range(self.args.epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            for batch_idx, (features, labels) in enumerate(train_loader):
                features = features.to(self.device)
                labels = labels.to(self.device)
                
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                model_outputs = model(features)
                emotion_logits = model_outputs['emotion_logits']
                speaker_logits = model_outputs.get('speaker_logits', None)
                
                # è®¡ç®—æƒ…æ„Ÿåˆ†ç±»æŸå¤±
                emotion_loss = criterion(emotion_logits, labels)
                
                # è®¡ç®—å¯¹æŠ—æŸå¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                total_loss = emotion_loss
                if speaker_logits is not None and self.args.speaker_adversarial:
                    # åˆ›å»ºå‡çš„è¯´è¯äººæ ‡ç­¾ï¼ˆéšæœºåˆ†é…ï¼‰
                    fake_speaker_labels = torch.randint(0, 10, (labels.size(0),), device=labels.device)
                    speaker_loss = criterion(speaker_logits, fake_speaker_labels)
                    total_loss = emotion_loss + self.args.adversarial_weight * speaker_loss
                
                loss = total_loss
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), self.args.max_grad_norm)
                
                optimizer.step()
                
                # ç»Ÿè®¡
                train_loss += loss.item()
                _, predicted = torch.max(emotion_logits.data, 1)
                train_total += labels.size(0)
                train_correct += (predicted == labels).sum().item()
                
                if batch_idx % self.args.log_interval == 0:
                    print(f'  Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for features, labels in val_loader:
                    features = features.to(self.device)
                    labels = labels.to(self.device)
                    
                    model_outputs = model(features)
                    emotion_logits = model_outputs['emotion_logits']
                    loss = criterion(emotion_logits, labels)
                    
                    val_loss += loss.item()
                    _, predicted = torch.max(emotion_logits.data, 1)
                    val_total += labels.size(0)
                    val_correct += (predicted == labels).sum().item()
            
            # è®¡ç®—å‡†ç¡®ç‡
            train_acc = 100.0 * train_correct / train_total
            val_acc = 100.0 * val_correct / val_total
            
            # æ›´æ–°å†å²
            train_history['loss'].append(train_loss / len(train_loader))
            train_history['acc'].append(train_acc)
            val_history['loss'].append(val_loss / len(val_loader))
            val_history['acc'].append(val_acc)
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            
            print(f'Epoch {epoch+1}/{self.args.epochs}:')
            print(f'  è®­ç»ƒ - Loss: {train_loss/len(train_loader):.4f}, Acc: {train_acc:.2f}%')
            print(f'  éªŒè¯ - Loss: {val_loss/len(val_loader):.4f}, Acc: {val_acc:.2f}%')
            print(f'  å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.6f}')
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_model_state = model.state_dict().copy()
                patience_counter = 0
                print(f'  âœ… æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%')
            else:
                patience_counter += 1
                
            # æ—©åœ
            if patience_counter >= self.args.patience:
                print(f'  â¹ï¸  æ—©åœè§¦å‘ï¼ŒéªŒè¯å‡†ç¡®ç‡è¿ç»­{self.args.patience}è½®æœªæå‡')
                break
            
            print('-' * 60)
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
            print(f"âœ… å·²åŠ è½½æœ€ä½³æ¨¡å‹ (éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%)")
        
        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(self.exp_dir, 'best_enhanced_model.pth')
        torch.save({
            'model_state_dict': model.state_dict(),
            'args': self.args,
            'best_val_acc': best_val_acc,
            'train_history': train_history,
            'val_history': val_history
        }, model_path)
        
        # ä¿å­˜ä¸ºpickleæ ¼å¼ï¼ˆå…¼å®¹GUIï¼‰
        model_pkl_path = os.path.join(self.exp_dir, 'best_enhanced_model.pkl')
        with open(model_pkl_path, 'wb') as f:
            pickle.dump({
                'model_state_dict': model.state_dict(),
                'args': self.args,
                'best_val_acc': best_val_acc
            }, f)
        
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        print(f"ğŸ’¾ å…¼å®¹æ ¼å¼å·²ä¿å­˜: {model_pkl_path}")
        
        # æµ‹è¯•æ¨¡å‹
        test_acc, test_f1, test_report = self.evaluate_model(model, test_loader)
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves(train_history, val_history)
        
        return {
            'model': model,
            'best_val_acc': best_val_acc,
            'test_acc': test_acc,
            'test_f1': test_f1,
            'test_report': test_report,
            'train_history': train_history,
            'val_history': val_history
        }
    
    def evaluate_model(self, model, test_loader):
        """è¯„ä¼°æ¨¡å‹"""
        print("\nğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        model.eval()
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for features, labels in test_loader:
                features = features.to(self.device)
                labels = labels.to(self.device)
                
                model_outputs = model(features)
                emotion_logits = model_outputs['emotion_logits']
                _, predicted = torch.max(emotion_logits.data, 1)
                
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        test_acc = accuracy_score(all_labels, all_predictions) * 100
        test_f1 = f1_score(all_labels, all_predictions, average='weighted') * 100
        
        # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
        test_report = classification_report(
            all_labels, all_predictions,
            target_names=[self.emotion_labels[i] for i in range(4)],
            digits=4
        )
        
        print(f"ğŸ¯ æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
        print(f"ğŸ¯ æµ‹è¯•F1åˆ†æ•°: {test_f1:.2f}%")
        print("\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(test_report)
        
        # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        self.plot_confusion_matrix(all_labels, all_predictions)
        
        return test_acc, test_f1, test_report
    
    def plot_training_curves(self, train_history, val_history):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # æŸå¤±æ›²çº¿
        ax1.plot(train_history['loss'], label='è®­ç»ƒæŸå¤±', color='blue')
        ax1.plot(val_history['loss'], label='éªŒè¯æŸå¤±', color='red')
        ax1.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(train_history['acc'], label='è®­ç»ƒå‡†ç¡®ç‡', color='blue')
        ax2.plot(val_history['acc'], label='éªŒè¯å‡†ç¡®ç‡', color='red')
        ax2.set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.exp_dir, 'training_curves.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {os.path.join(self.exp_dir, 'training_curves.png')}")
    
    def plot_confusion_matrix(self, true_labels, pred_labels):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        cm = confusion_matrix(true_labels, pred_labels)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=[self.emotion_labels[i] for i in range(4)],
                   yticklabels=[self.emotion_labels[i] for i in range(4)])
        plt.title('æ··æ·†çŸ©é˜µ')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.exp_dir, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜: {os.path.join(self.exp_dir, 'confusion_matrix.png')}")

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='ç®€åŒ–çš„å¢å¼ºIEMOCAPè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«è®­ç»ƒ')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--data_path', type=str, default='./Train_data_org.pickle', help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=16, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_workers', type=int, default=4, help='æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--hidden_layer', type=int, default=128, help='éšè—å±‚å¤§å°')
    parser.add_argument('--out_class', type=int, default=4, help='è¾“å‡ºç±»åˆ«æ•°')
    parser.add_argument('--dia_layers', type=int, default=2, help='GRUå±‚æ•°')
    parser.add_argument('--dropout', type=float, default=0.3, help='Dropoutç‡')
    parser.add_argument('--attention', type=bool, default=True, help='æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶')
    parser.add_argument('--speaker_norm', type=bool, default=True, help='æ˜¯å¦ä½¿ç”¨è¯´è¯äººå½’ä¸€åŒ–')
    parser.add_argument('--speaker_adversarial', type=bool, default=True, help='æ˜¯å¦ä½¿ç”¨è¯´è¯äººå¯¹æŠ—è®­ç»ƒ')
    parser.add_argument('--freeze_layers', type=int, default=6, help='å†»ç»“çš„HuBERTå±‚æ•°')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-4, help='æƒé‡è¡°å‡')
    parser.add_argument('--max_grad_norm', type=float, default=1.0, help='æ¢¯åº¦è£å‰ªæœ€å¤§èŒƒæ•°')
    parser.add_argument('--patience', type=int, default=10, help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--T_0', type=int, default=10, help='ä½™å¼¦é€€ç«è°ƒåº¦å™¨å‘¨æœŸ')
    
    # å¯¹æŠ—è®­ç»ƒå‚æ•°
    parser.add_argument('--adversarial_weight', type=float, default=0.1, help='å¯¹æŠ—æŸå¤±æƒé‡')
    parser.add_argument('--adversarial_warmup', type=int, default=10, help='å¯¹æŠ—è®­ç»ƒé¢„çƒ­è½®æ•°')
    parser.add_argument('--l2_reg', type=float, default=1e-5, help='L2æ­£åˆ™åŒ–æƒé‡')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--cuda', type=bool, default=True, help='æ˜¯å¦ä½¿ç”¨CUDA')
    parser.add_argument('--log_interval', type=int, default=50, help='æ—¥å¿—è¾“å‡ºé—´éš”')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    
    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ç®€åŒ–çš„å¢å¼ºIEMOCAPè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«è®­ç»ƒç³»ç»Ÿ")
    print("=" * 60)
    
    # è§£æå‚æ•°
    args = parse_args()
    
    # æ˜¾ç¤ºé…ç½®
    print("ğŸ“‹ è®­ç»ƒé…ç½®:")
    for key, value in vars(args).items():
        print(f"   {key}: {value}")
    print("=" * 60)
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = SimpleEnhancedTrainer(args)
        
        # å¼€å§‹è®­ç»ƒ
        results = trainer.train_model()
        
        print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"âœ… æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {results['best_val_acc']:.2f}%")
        print(f"âœ… æµ‹è¯•å‡†ç¡®ç‡: {results['test_acc']:.2f}%")
        print(f"âœ… æµ‹è¯•F1åˆ†æ•°: {results['test_f1']:.2f}%")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {trainer.exp_dir}")
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
