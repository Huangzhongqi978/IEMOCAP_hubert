#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯´è¯äººæ— å…³æ•°æ®å¤„ç†æ¨¡å—
å®ç°IEMOCAPæ•°æ®é›†çš„è¯´è¯äººæ— å…³åˆ’åˆ†å’Œæ•°æ®å¢å¼º
"""

import pickle
import numpy as np
import torch
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
from collections import defaultdict, Counter
import random
import re
import os

class SpeakerIndependentDataLoader:
    """è¯´è¯äººæ— å…³æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, data_path='./Train_data_org.pickle', test_ratio=0.2, val_ratio=0.1):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
        """
        self.data_path = data_path
        self.test_ratio = test_ratio
        self.val_ratio = val_ratio
        
        # IEMOCAPè¯´è¯äººä¿¡æ¯
        self.sessions = ['Session1', 'Session2', 'Session3', 'Session4', 'Session5']
        self.speakers_per_session = {
            'Session1': ['Ses01F', 'Ses01M'],
            'Session2': ['Ses02F', 'Ses02M'],
            'Session3': ['Ses03F', 'Ses03M'], 
            'Session4': ['Ses04F', 'Ses04M'],
            'Session5': ['Ses05F', 'Ses05M']
        }
        
        # æƒ…æ„Ÿæ ‡ç­¾æ˜ å°„
        self.emotion_mapping = {
            'ang': 0,  # Angry
            'hap': 1,  # Happy
            'neu': 2,  # Neutral  
            'sad': 3   # Sad
        }
        self.emotion_labels = {0: 'Angry', 1: 'Happy', 2: 'Neutral', 3: 'Sad'}
        
        self.data = None
        self.speaker_data = defaultdict(list)
        self.load_data()
        
    def extract_speaker_from_filename(self, filename):
        """ä»æ–‡ä»¶åæå–è¯´è¯äººä¿¡æ¯"""
        # IEMOCAPæ–‡ä»¶åæ ¼å¼: Ses01F_impro01_F000.wav
        match = re.match(r'(Ses\d+[FM])_', filename)
        if match:
            return match.group(1)
        return None
    
    def load_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        print("ğŸ“ åŠ è½½IEMOCAPæ•°æ®...")
        
        with open(self.data_path, 'rb') as f:
            self.data = pickle.load(f)
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(self.data)} ä¸ªæ ·æœ¬")
        
        # æŒ‰è¯´è¯äººåˆ†ç»„æ•°æ®
        for i, sample in enumerate(self.data):
            # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
            if isinstance(sample, list):
                # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼ [features, label, filename]
                if len(sample) >= 3:
                    filename = sample[2] if isinstance(sample[2], str) else ''
                    features = sample[0]
                    label = sample[1]
                    sample_dict = {'features': features, 'emotion': label, 'id': filename}
                else:
                    print(f"âš ï¸ è·³è¿‡æ ¼å¼ä¸æ­£ç¡®çš„æ ·æœ¬ {i}: {type(sample)}")
                    continue
            elif isinstance(sample, dict):
                # å¦‚æœå·²ç»æ˜¯å­—å…¸æ ¼å¼
                filename = sample.get('id', '')
                sample_dict = sample
            else:
                print(f"âš ï¸ è·³è¿‡æœªçŸ¥æ ¼å¼çš„æ ·æœ¬ {i}: {type(sample)}")
                continue
                
            speaker = self.extract_speaker_from_filename(filename)
            
            if speaker:
                # ç¡®ä¿æƒ…æ„Ÿæ ‡ç­¾åœ¨æˆ‘ä»¬çš„æ˜ å°„ä¸­
                emotion = sample_dict.get('emotion', -1)
                if emotion in [0, 1, 2, 3]:  # åªä¿ç•™å››ä¸ªä¸»è¦æƒ…æ„Ÿ
                    sample_dict['speaker'] = speaker
                    self.speaker_data[speaker].append(sample_dict)
        
        # ç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š è¯´è¯äººæ•°æ®åˆ†å¸ƒ:")
        total_samples = 0
        emotion_counts = Counter()
        
        for speaker, samples in self.speaker_data.items():
            emotion_dist = Counter([s['emotion'] for s in samples])
            print(f"  {speaker}: {len(samples)} æ ·æœ¬ - {dict(emotion_dist)}")
            total_samples += len(samples)
            emotion_counts.update(emotion_dist)
        
        print(f"\nğŸ“ˆ æ€»ä½“æƒ…æ„Ÿåˆ†å¸ƒ: {dict(emotion_counts)}")
        print(f"ğŸ“Š æœ‰æ•ˆæ ·æœ¬æ€»æ•°: {total_samples}")
    
    def create_speaker_independent_splits(self, fold_idx=0, n_folds=5):
        """
        åˆ›å»ºè¯´è¯äººæ— å…³çš„æ•°æ®åˆ’åˆ†
        
        Args:
            fold_idx: å½“å‰æŠ˜æ•°
            n_folds: æ€»æŠ˜æ•°
            
        Returns:
            train_data, val_data, test_data: è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®
        """
        print(f"\nğŸ”„ åˆ›å»ºç¬¬ {fold_idx+1}/{n_folds} æŠ˜çš„è¯´è¯äººæ— å…³åˆ’åˆ†...")
        
        speakers = list(self.speaker_data.keys())
        speakers.sort()  # ç¡®ä¿å¯é‡å¤æ€§
        
        # ä½¿ç”¨å›ºå®šçš„è¯´è¯äººåˆ’åˆ†ç­–ç•¥
        n_test_speakers = max(1, len(speakers) // n_folds)
        
        # è®¡ç®—æµ‹è¯•è¯´è¯äºº
        start_idx = fold_idx * n_test_speakers
        end_idx = min(start_idx + n_test_speakers, len(speakers))
        test_speakers = speakers[start_idx:end_idx]
        
        # å‰©ä½™è¯´è¯äººç”¨äºè®­ç»ƒå’ŒéªŒè¯
        remaining_speakers = [s for s in speakers if s not in test_speakers]
        
        # ä»å‰©ä½™è¯´è¯äººä¸­é€‰æ‹©éªŒè¯è¯´è¯äºº
        n_val_speakers = max(1, len(remaining_speakers) // 5)
        val_speakers = remaining_speakers[:n_val_speakers]
        train_speakers = remaining_speakers[n_val_speakers:]
        
        print(f"ğŸ¯ è®­ç»ƒè¯´è¯äºº: {train_speakers}")
        print(f"ğŸ¯ éªŒè¯è¯´è¯äºº: {val_speakers}")
        print(f"ğŸ¯ æµ‹è¯•è¯´è¯äºº: {test_speakers}")
        
        # æ”¶é›†æ•°æ®
        train_data = []
        val_data = []
        test_data = []
        
        for speaker in train_speakers:
            train_data.extend(self.speaker_data[speaker])
        
        for speaker in val_speakers:
            val_data.extend(self.speaker_data[speaker])
            
        for speaker in test_speakers:
            test_data.extend(self.speaker_data[speaker])
        
        # æ•°æ®å¢å¼ºï¼ˆä»…å¯¹è®­ç»ƒé›†ï¼‰
        train_data = self.augment_training_data(train_data)
        
        print(f"ğŸ“Š æ•°æ®åˆ’åˆ†ç»“æœ:")
        print(f"  è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
        print(f"  éªŒè¯é›†: {len(val_data)} æ ·æœ¬")
        print(f"  æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬")
        
        return train_data, val_data, test_data
    
    def augment_training_data(self, train_data):
        """
        è®­ç»ƒæ•°æ®å¢å¼º
        
        Args:
            train_data: åŸå§‹è®­ç»ƒæ•°æ®
            
        Returns:
            augmented_data: å¢å¼ºåçš„è®­ç»ƒæ•°æ®
        """
        print("ğŸ”§ åº”ç”¨è®­ç»ƒæ•°æ®å¢å¼º...")
        
        augmented_data = train_data.copy()
        
        # ç»Ÿè®¡å„æƒ…æ„Ÿç±»åˆ«çš„æ ·æœ¬æ•°
        emotion_counts = Counter([sample['emotion'] for sample in train_data])
        max_count = max(emotion_counts.values())
        
        # å¯¹å°‘æ•°ç±»è¿›è¡Œè¿‡é‡‡æ ·
        for emotion, count in emotion_counts.items():
            if count < max_count * 0.8:  # å¦‚æœæŸç±»åˆ«æ ·æœ¬æ•°å°‘äºæœ€å¤šç±»åˆ«çš„80%
                emotion_samples = [s for s in train_data if s['emotion'] == emotion]
                
                # è®¡ç®—éœ€è¦å¢å¼ºçš„æ•°é‡
                target_count = int(max_count * 0.8)
                need_augment = target_count - count
                
                if need_augment > 0:
                    # éšæœºé€‰æ‹©æ ·æœ¬è¿›è¡Œå¢å¼º
                    augment_samples = np.random.choice(emotion_samples, 
                                                     size=min(need_augment, len(emotion_samples)), 
                                                     replace=True)
                    
                    for sample in augment_samples:
                        # åˆ›å»ºå¢å¼ºæ ·æœ¬ï¼ˆè¿™é‡Œç®€å•å¤åˆ¶ï¼Œå®é™…å¯ä»¥åŠ å…¥å™ªå£°ç­‰ï¼‰
                        aug_sample = sample.copy()
                        aug_sample['id'] = aug_sample['id'] + '_aug'
                        augmented_data.append(aug_sample)
        
        # ç»Ÿè®¡å¢å¼ºåçš„åˆ†å¸ƒ
        final_emotion_counts = Counter([sample['emotion'] for sample in augmented_data])
        print(f"ğŸ“ˆ æ•°æ®å¢å¼ºåæƒ…æ„Ÿåˆ†å¸ƒ: {dict(final_emotion_counts)}")
        
        return augmented_data
    
    def create_data_loaders(self, train_data, val_data, test_data, batch_size=32, num_workers=4):
        """
        åˆ›å»ºPyTorchæ•°æ®åŠ è½½å™¨
        
        Args:
            train_data, val_data, test_data: æ•°æ®åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            num_workers: å·¥ä½œè¿›ç¨‹æ•°
            
        Returns:
            train_loader, val_loader, test_loader: PyTorchæ•°æ®åŠ è½½å™¨
        """
        from torch.utils.data import DataLoader
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = IEMOCAPDataset(train_data, is_training=True)
        val_dataset = IEMOCAPDataset(val_data, is_training=False)
        test_dataset = IEMOCAPDataset(test_data, is_training=False)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True, 
            num_workers=num_workers,
            pin_memory=True,
            drop_last=True
        )
        
        val_loader = DataLoader(
            val_dataset, 
            batch_size=batch_size, 
            shuffle=False, 
            num_workers=num_workers,
            pin_memory=True
        )
        
        test_loader = DataLoader(
            test_dataset, 
            batch_size=batch_size, 
            shuffle=False, 
            num_workers=num_workers,
            pin_memory=True
        )
        
        return train_loader, val_loader, test_loader

class IEMOCAPDataset(torch.utils.data.Dataset):
    """IEMOCAPæ•°æ®é›†ç±»"""
    
    def __init__(self, data_list, is_training=False, max_length=None):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data_list: æ•°æ®æ ·æœ¬åˆ—è¡¨
            is_training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.data_list = data_list
        self.is_training = is_training
        self.max_length = max_length
        
        # æ•°æ®å¢å¼ºå‚æ•°
        self.noise_factor = 0.005
        self.time_stretch_factor = 0.1
        
    def __len__(self):
        return len(self.data_list)
    
    def __getitem__(self, idx):
        """è·å–å•ä¸ªæ ·æœ¬"""
        sample = self.data_list[idx]
        
        # è·å–éŸ³é¢‘ç‰¹å¾
        wav_encodings = sample['wav_encodings']
        if isinstance(wav_encodings, torch.Tensor):
            audio_features = wav_encodings.squeeze()
        else:
            audio_features = torch.tensor(wav_encodings, dtype=torch.float32).squeeze()
        
        # è·å–æ ‡ç­¾å’Œå…¶ä»–ä¿¡æ¯
        emotion_label = torch.tensor(sample['emotion'], dtype=torch.long)
        sample_id = sample['id']
        speaker = sample.get('speaker', 'unknown')
        
        # è·å–è¯´è¯äººæ ‡ç­¾ï¼ˆç”¨äºå¯¹æŠ—è®­ç»ƒï¼‰
        speaker_label = self._get_speaker_label(speaker)
        
        # è®­ç»ƒæ—¶åº”ç”¨æ•°æ®å¢å¼º
        if self.is_training:
            audio_features = self._apply_augmentation(audio_features)
        
        # åºåˆ—é•¿åº¦å¤„ç†
        seq_length = audio_features.shape[0] if audio_features.dim() > 1 else 1
        
        return {
            'audio_features': audio_features,
            'emotion_label': emotion_label,
            'speaker_label': speaker_label,
            'seq_length': seq_length,
            'sample_id': sample_id,
            'speaker': speaker
        }
    
    def _get_speaker_label(self, speaker):
        """è·å–è¯´è¯äººæ ‡ç­¾"""
        speaker_mapping = {
            'Ses01F': 0, 'Ses01M': 1,
            'Ses02F': 2, 'Ses02M': 3,
            'Ses03F': 4, 'Ses03M': 5,
            'Ses04F': 6, 'Ses04M': 7,
            'Ses05F': 8, 'Ses05M': 9
        }
        return torch.tensor(speaker_mapping.get(speaker, 0), dtype=torch.long)
    
    def _apply_augmentation(self, audio_features):
        """åº”ç”¨æ•°æ®å¢å¼º"""
        if not self.is_training:
            return audio_features
        
        # æ·»åŠ é«˜æ–¯å™ªå£°
        if random.random() < 0.3:
            noise = torch.randn_like(audio_features) * self.noise_factor
            audio_features = audio_features + noise
        
        # æ—¶é—´é®è”½ï¼ˆç±»ä¼¼SpecAugmentï¼‰
        if random.random() < 0.2 and audio_features.dim() > 1:
            seq_len = audio_features.shape[0]
            mask_len = int(seq_len * 0.1)  # é®è”½10%çš„æ—¶é—´æ­¥
            mask_start = random.randint(0, max(0, seq_len - mask_len))
            audio_features[mask_start:mask_start + mask_len] *= 0.1
        
        return audio_features

def collate_fn(batch):
    """è‡ªå®šä¹‰æ‰¹æ¬¡æ•´ç†å‡½æ•°"""
    # æå–å„ä¸ªå­—æ®µ
    audio_features = [item['audio_features'] for item in batch]
    emotion_labels = torch.stack([item['emotion_label'] for item in batch])
    speaker_labels = torch.stack([item['speaker_label'] for item in batch])
    seq_lengths = torch.tensor([item['seq_length'] for item in batch])
    sample_ids = [item['sample_id'] for item in batch]
    speakers = [item['speaker'] for item in batch]
    
    # å¯¹éŸ³é¢‘ç‰¹å¾è¿›è¡Œå¡«å……
    if audio_features[0].dim() > 1:
        # äºŒç»´ç‰¹å¾ï¼Œéœ€è¦åœ¨åºåˆ—ç»´åº¦å¡«å……
        max_len = max([feat.shape[0] for feat in audio_features])
        padded_features = []
        
        for feat in audio_features:
            if feat.shape[0] < max_len:
                pad_len = max_len - feat.shape[0]
                padded_feat = F.pad(feat, (0, 0, 0, pad_len))
            else:
                padded_feat = feat
            padded_features.append(padded_feat)
        
        audio_features = torch.stack(padded_features)
    else:
        # ä¸€ç»´ç‰¹å¾ï¼Œç›´æ¥å †å 
        audio_features = torch.stack(audio_features)
    
    return {
        'audio_features': audio_features,
        'emotion_labels': emotion_labels,
        'speaker_labels': speaker_labels,
        'seq_lengths': seq_lengths,
        'sample_ids': sample_ids,
        'speakers': speakers
    }

if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®åŠ è½½å™¨
    print("ğŸ§ª æµ‹è¯•è¯´è¯äººæ— å…³æ•°æ®åŠ è½½å™¨...")
    
    try:
        loader = SpeakerIndependentDataLoader()
        
        # æµ‹è¯•æ•°æ®åˆ’åˆ†
        for fold in range(3):  # æµ‹è¯•å‰3æŠ˜
            train_data, val_data, test_data = loader.create_speaker_independent_splits(fold, n_folds=5)
            print(f"âœ… ç¬¬ {fold+1} æŠ˜åˆ’åˆ†æˆåŠŸ")
        
        print("ğŸ‰ è¯´è¯äººæ— å…³æ•°æ®åŠ è½½å™¨æµ‹è¯•é€šè¿‡!")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
