import sys
import torch
import numpy as np
import pyaudio
import wave
import torchaudio
from PyQt5 import QtWidgets, QtCore, QtGui
from transformers import Wav2Vec2Processor, HubertModel
from models import SpeechRecognitionModel

# ====== æ¨¡å‹åŠ è½½ ======
label_map = {0: "neutral", 1: "happy", 2: "angry", 3: "sad"} 
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
hubert = HubertModel.from_pretrained("facebook/hubert-base-ls960")
hubert.eval()

class Cfg:
    dropout = 0.2
    dia_layers = 2
    hidden_layer = 256
    out_class = 4
    utt_insize = 768
    attention = True
    bid_flag = False
    batch_first = False
    cuda = False

cfg = Cfg()
model = SpeechRecognitionModel(cfg)
# ç›´æ¥ä½¿ç”¨torch.loadï¼Œä¸è®¾ç½®weights_onlyå‚æ•°
state_dict = torch.load("model.pkl", map_location="cpu")

model.load_state_dict(state_dict, strict=True)
model.eval()

# ====== å½•éŸ³å‡½æ•° ======
def record_audio(filename="temp.wav", record_seconds=3, rate=16000):
    chunk = 1024
    p = pyaudio.PyAudio()
    stream = p.open(format=pyaudio.paInt16,
                    channels=1,
                    rate=rate,
                    input=True,
                    frames_per_buffer=chunk)
    frames = []
    for i in range(0, int(rate / chunk * record_seconds)):
        data = stream.read(chunk)
        frames.append(data)
    stream.stop_stream()
    stream.close()
    p.terminate()
    wf = wave.open(filename, 'wb')
    wf.setnchannels(1)
    wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))
    wf.setframerate(rate)
    wf.writeframes(b''.join(frames))
    wf.close()
    return filename

# ====== ç‰¹å¾æå– ======
def extract_ssl_features_from_wav(wav_path, duration_sec=3):
    wav, sr = torchaudio.load(wav_path)
    target_length = int(duration_sec * sr)
    if wav.size(1) > target_length:
        wav = wav[:, :target_length]
    else:
        padding_length = target_length - wav.size(1)
        wav = torch.nn.functional.pad(wav, (0, padding_length))
    inputs = processor(wav, sampling_rate=sr, return_tensors="pt").input_values
    return inputs

# ====== é¢„æµ‹ ======
def predict(model, feat_tensor):
    with torch.no_grad():
        x_try = feat_tensor.squeeze(0)
        utt_out, _ = model(x_try)
        pred_label = int(torch.argmax(utt_out, dim=1).item())
        probs = torch.softmax(utt_out, dim=-1).cpu().numpy().squeeze(0)
    return pred_label, probs

# ====== PyQt ç•Œé¢ ======
class EmotionApp(QtWidgets.QWidget):
    def __init__(self):
        super().__init__()
        self.initUI()

    def initUI(self):
        self.setWindowTitle("å®æ—¶è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«")
        self.resize(500, 300)
        self.setStyleSheet("""
            QWidget {
                background-color: #f5f5f5;
                font-family: "Microsoft YaHei";
            }
            QPushButton {
                background-color: #4CAF50;
                color: white;
                font-size: 18px;
                padding: 10px;
                border-radius: 15px;
            }
            QPushButton:hover {
                background-color: #45a049;
            }
            QLabel {
                font-size: 20px;
                color: #333;
            }
        """)

        # æ ‡é¢˜
        self.title = QtWidgets.QLabel("ğŸ¤ å®æ—¶è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«", self)
        self.title.setAlignment(QtCore.Qt.AlignCenter)
        self.title.setStyleSheet("font-size: 24px; font-weight: bold;")

        # çŠ¶æ€æ ‡ç­¾
        self.status = QtWidgets.QLabel("ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®å¼€å§‹å½•éŸ³", self)
        self.status.setAlignment(QtCore.Qt.AlignCenter)

        # æŒ‰é’®
        self.btn_record = QtWidgets.QPushButton("ğŸ™ å¼€å§‹å½•éŸ³å¹¶è¯†åˆ«", self)
        self.btn_record.clicked.connect(self.record_and_predict)

        # ç»“æœ
        self.result = QtWidgets.QLabel("", self)
        self.result.setAlignment(QtCore.Qt.AlignCenter)
        self.result.setStyleSheet("font-size: 22px; font-weight: bold; color: #2E86C1;")

        # å¸ƒå±€
        vbox = QtWidgets.QVBoxLayout()
        vbox.addWidget(self.title)
        vbox.addSpacing(10)
        vbox.addWidget(self.status)
        vbox.addSpacing(15)
        vbox.addWidget(self.btn_record)
        vbox.addSpacing(20)
        vbox.addWidget(self.result)
        self.setLayout(vbox)

    def record_and_predict(self):
        self.status.setText("ğŸ¤ æ­£åœ¨å½•éŸ³ä¸­...")
        QtWidgets.QApplication.processEvents()

        wav_path = record_audio("temp.wav", record_seconds=3)
        feats = extract_ssl_features_from_wav(wav_path)
        pred, probs = predict(model, feats)

        emotion = label_map[pred]
        confidence = probs[pred]

        self.result.setText(f"ç»“æœ: {emotion} ({confidence:.2f})")
        self.status.setText("âœ… å½•éŸ³å®Œæˆ")

if __name__ == '__main__':
    app = QtWidgets.QApplication(sys.argv)
    ex = EmotionApp()
    ex.show()
    sys.exit(app.exec_())
