#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è·¨è¯´è¯äººæ€§èƒ½è¯„ä¼°è„šæœ¬
å¯¹æ¯”åŸå§‹æ¨¡å‹å’Œå¢å¼ºæ¨¡å‹åœ¨è·¨è¯´è¯äººæµ‹è¯•ä¸­çš„æ€§èƒ½
"""

import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from collections import defaultdict, Counter
import os
import json
from datetime import datetime
import argparse

# å¯¼å…¥æ¨¡å‹
from models.enhanced_gru import create_enhanced_model
from models.GRU import SpeechRecognitionModel  # åŸå§‹æ¨¡å‹
from utils.speaker_independent_data import SpeakerIndependentDataLoader, collate_fn
from torch.utils.data import DataLoader

import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans', 'Arial Unicode MS']
matplotlib.rcParams['axes.unicode_minus'] = False

class SpeakerIndependenceEvaluator:
    """è·¨è¯´è¯äººç‹¬ç«‹æ€§è¯„ä¼°å™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device('cuda' if torch.cuda.is_available() and args.cuda else 'cpu')
        
        # æƒ…æ„Ÿæ ‡ç­¾
        self.emotion_labels = {0: 'Angry', 1: 'Happy', 2: 'Neutral', 3: 'Sad'}
        self.emotion_colors = ['#e74c3c', '#f1c40f', '#95a5a6', '#3498db']
        
        # è¯´è¯äººæ ‡ç­¾
        self.speaker_labels = {
            0: 'Ses01F', 1: 'Ses01M', 2: 'Ses02F', 3: 'Ses02M', 4: 'Ses03F',
            5: 'Ses03M', 6: 'Ses04F', 7: 'Ses04M', 8: 'Ses05F', 9: 'Ses05M'
        }
        
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        self.data_loader = SpeakerIndependentDataLoader(args.data_path)
        
        # åˆ›å»ºç»“æœç›®å½•
        self.eval_dir = os.path.join('evaluations', f'speaker_independence_{datetime.now().strftime("%Y%m%d_%H%M%S")}')
        os.makedirs(self.eval_dir, exist_ok=True)
        
        print(f"ğŸ“ è¯„ä¼°ç»“æœå°†ä¿å­˜åˆ°: {self.eval_dir}")
    
    def load_model(self, model_path, model_type='enhanced'):
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ“¥ åŠ è½½{model_type}æ¨¡å‹: {model_path}")
        
        checkpoint = torch.load(model_path, map_location=self.device)
        
        if model_type == 'enhanced':
            model = create_enhanced_model(self.args)
        else:
            model = SpeechRecognitionModel(self.args)
        
        model.load_state_dict(checkpoint['model_state_dict'])
        model = model.to(self.device)
        model.eval()
        
        print(f"âœ… {model_type}æ¨¡å‹åŠ è½½æˆåŠŸ")
        return model
    
    def evaluate_model_on_speakers(self, model, test_data, model_name="Model"):
        """è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè¯´è¯äººä¸Šçš„æ€§èƒ½"""
        print(f"ğŸ” è¯„ä¼°{model_name}åœ¨è·¨è¯´è¯äººæµ‹è¯•ä¸­çš„æ€§èƒ½...")
        
        # æŒ‰è¯´è¯äººåˆ†ç»„æµ‹è¯•æ•°æ®
        speaker_data = defaultdict(list)
        for sample in test_data:
            speaker = sample.get('speaker', 'unknown')
            speaker_data[speaker].append(sample)
        
        speaker_results = {}
        all_predictions = []
        all_targets = []
        all_speakers = []
        
        for speaker, samples in speaker_data.items():
            if len(samples) == 0:
                continue
                
            # åˆ›å»ºå•ä¸ªè¯´è¯äººçš„æ•°æ®åŠ è½½å™¨
            speaker_loader = DataLoader(
                samples,
                batch_size=self.args.batch_size,
                shuffle=False,
                collate_fn=collate_fn,
                num_workers=0
            )
            
            # è¯„ä¼°
            speaker_preds = []
            speaker_targets = []
            
            with torch.no_grad():
                for batch in speaker_loader:
                    audio_features = batch['audio_features'].to(self.device)
                    emotion_targets = batch['emotion_labels'].to(self.device)
                    
                    if hasattr(model, 'utterance_net'):
                        # å¢å¼ºæ¨¡å‹
                        outputs = model(audio_features)
                        predictions = torch.argmax(outputs['emotion_logits'], dim=1)
                    else:
                        # åŸå§‹æ¨¡å‹
                        outputs, _ = model(audio_features)
                        predictions = torch.argmax(outputs, dim=1)
                    
                    speaker_preds.extend(predictions.cpu().numpy())
                    speaker_targets.extend(emotion_targets.cpu().numpy())
            
            # è®¡ç®—è¯¥è¯´è¯äººçš„æ€§èƒ½æŒ‡æ ‡
            accuracy = accuracy_score(speaker_targets, speaker_preds)
            f1 = f1_score(speaker_targets, speaker_preds, average='weighted')
            f1_per_class = f1_score(speaker_targets, speaker_preds, average=None)
            
            speaker_results[speaker] = {
                'accuracy': accuracy,
                'f1_score': f1,
                'f1_per_class': f1_per_class,
                'sample_count': len(speaker_targets),
                'predictions': speaker_preds,
                'targets': speaker_targets
            }
            
            # æ”¶é›†æ‰€æœ‰ç»“æœ
            all_predictions.extend(speaker_preds)
            all_targets.extend(speaker_targets)
            all_speakers.extend([speaker] * len(speaker_preds))
            
            print(f"  {speaker}: å‡†ç¡®ç‡={accuracy:.4f}, F1={f1:.4f}, æ ·æœ¬æ•°={len(speaker_targets)}")
        
        # è®¡ç®—æ€»ä½“æ€§èƒ½
        overall_accuracy = accuracy_score(all_targets, all_predictions)
        overall_f1 = f1_score(all_targets, all_predictions, average='weighted')
        
        print(f"ğŸ¯ {model_name}æ€»ä½“æ€§èƒ½: å‡†ç¡®ç‡={overall_accuracy:.4f}, F1={overall_f1:.4f}")
        
        return {
            'speaker_results': speaker_results,
            'overall_accuracy': overall_accuracy,
            'overall_f1': overall_f1,
            'all_predictions': all_predictions,
            'all_targets': all_targets,
            'all_speakers': all_speakers
        }
    
    def compare_models(self, enhanced_model_path, original_model_path=None):
        """å¯¹æ¯”å¢å¼ºæ¨¡å‹å’ŒåŸå§‹æ¨¡å‹"""
        print("ğŸ”„ å¼€å§‹æ¨¡å‹å¯¹æ¯”è¯„ä¼°...")
        
        # åˆ›å»ºè·¨è¯´è¯äººæµ‹è¯•é›†
        _, _, test_data = self.data_loader.create_speaker_independent_splits(0, n_folds=5)
        
        # è¯„ä¼°å¢å¼ºæ¨¡å‹
        enhanced_model = self.load_model(enhanced_model_path, 'enhanced')
        enhanced_results = self.evaluate_model_on_speakers(enhanced_model, test_data, "å¢å¼ºæ¨¡å‹")
        
        # è¯„ä¼°åŸå§‹æ¨¡å‹ï¼ˆå¦‚æœæä¾›ï¼‰
        original_results = None
        if original_model_path and os.path.exists(original_model_path):
            original_model = self.load_model(original_model_path, 'original')
            original_results = self.evaluate_model_on_speakers(original_model, test_data, "åŸå§‹æ¨¡å‹")
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        comparison_results = {
            'enhanced': enhanced_results,
            'original': original_results,
            'test_data_info': {
                'total_samples': len(test_data),
                'speaker_distribution': self._get_speaker_distribution(test_data)
            }
        }
        
        # å¯è§†åŒ–å¯¹æ¯”ç»“æœ
        self.visualize_comparison(comparison_results)
        
        # ä¿å­˜ç»“æœ
        self.save_results(comparison_results)
        
        return comparison_results
    
    def _get_speaker_distribution(self, test_data):
        """è·å–æµ‹è¯•æ•°æ®ä¸­çš„è¯´è¯äººåˆ†å¸ƒ"""
        speaker_counts = Counter()
        emotion_by_speaker = defaultdict(Counter)
        
        for sample in test_data:
            speaker = sample.get('speaker', 'unknown')
            emotion = sample.get('emotion', -1)
            
            speaker_counts[speaker] += 1
            emotion_by_speaker[speaker][emotion] += 1
        
        return {
            'speaker_counts': dict(speaker_counts),
            'emotion_by_speaker': {k: dict(v) for k, v in emotion_by_speaker.items()}
        }
    
    def visualize_comparison(self, comparison_results):
        """å¯è§†åŒ–å¯¹æ¯”ç»“æœ"""
        enhanced_results = comparison_results['enhanced']
        original_results = comparison_results['original']
        
        # 1. è¯´è¯äººæ€§èƒ½å¯¹æ¯”
        self.plot_speaker_performance_comparison(enhanced_results, original_results)
        
        # 2. æ··æ·†çŸ©é˜µå¯¹æ¯”
        self.plot_confusion_matrix_comparison(enhanced_results, original_results)
        
        # 3. æƒ…æ„Ÿç±»åˆ«æ€§èƒ½å¯¹æ¯”
        self.plot_emotion_performance_comparison(enhanced_results, original_results)
        
        # 4. è¯´è¯äººåè§åˆ†æ
        self.plot_speaker_bias_analysis(enhanced_results, original_results)
    
    def plot_speaker_performance_comparison(self, enhanced_results, original_results):
        """ç»˜åˆ¶è¯´è¯äººæ€§èƒ½å¯¹æ¯”"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        speakers = list(enhanced_results['speaker_results'].keys())
        enhanced_acc = [enhanced_results['speaker_results'][s]['accuracy'] for s in speakers]
        enhanced_f1 = [enhanced_results['speaker_results'][s]['f1_score'] for s in speakers]
        
        x = np.arange(len(speakers))
        width = 0.35
        
        # å‡†ç¡®ç‡å¯¹æ¯”
        ax1.bar(x - width/2, enhanced_acc, width, label='å¢å¼ºæ¨¡å‹', alpha=0.8, color='skyblue')
        
        if original_results:
            original_acc = [original_results['speaker_results'][s]['accuracy'] for s in speakers]
            ax1.bar(x + width/2, original_acc, width, label='åŸå§‹æ¨¡å‹', alpha=0.8, color='lightcoral')
        
        ax1.set_xlabel('è¯´è¯äºº')
        ax1.set_ylabel('å‡†ç¡®ç‡')
        ax1.set_title('å„è¯´è¯äººå‡†ç¡®ç‡å¯¹æ¯”')
        ax1.set_xticks(x)
        ax1.set_xticklabels(speakers, rotation=45)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # F1åˆ†æ•°å¯¹æ¯”
        ax2.bar(x - width/2, enhanced_f1, width, label='å¢å¼ºæ¨¡å‹', alpha=0.8, color='lightgreen')
        
        if original_results:
            original_f1 = [original_results['speaker_results'][s]['f1_score'] for s in speakers]
            ax2.bar(x + width/2, original_f1, width, label='åŸå§‹æ¨¡å‹', alpha=0.8, color='orange')
        
        ax2.set_xlabel('è¯´è¯äºº')
        ax2.set_ylabel('F1åˆ†æ•°')
        ax2.set_title('å„è¯´è¯äººF1åˆ†æ•°å¯¹æ¯”')
        ax2.set_xticks(x)
        ax2.set_xticklabels(speakers, rotation=45)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.eval_dir, 'speaker_performance_comparison.png'),
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_confusion_matrix_comparison(self, enhanced_results, original_results):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µå¯¹æ¯”"""
        if original_results is None:
            # åªæœ‰å¢å¼ºæ¨¡å‹çš„æ··æ·†çŸ©é˜µ
            fig, ax = plt.subplots(1, 1, figsize=(8, 6))
            
            cm = confusion_matrix(enhanced_results['all_targets'], enhanced_results['all_predictions'])
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                       xticklabels=self.emotion_labels.values(),
                       yticklabels=self.emotion_labels.values())
            ax.set_title(f'å¢å¼ºæ¨¡å‹æ··æ·†çŸ©é˜µ\\nå‡†ç¡®ç‡: {enhanced_results["overall_accuracy"]:.4f}')
            ax.set_xlabel('é¢„æµ‹æ ‡ç­¾')
            ax.set_ylabel('çœŸå®æ ‡ç­¾')
        else:
            # å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„æ··æ·†çŸ©é˜µ
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
            
            # å¢å¼ºæ¨¡å‹
            cm1 = confusion_matrix(enhanced_results['all_targets'], enhanced_results['all_predictions'])
            sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues', ax=ax1,
                       xticklabels=self.emotion_labels.values(),
                       yticklabels=self.emotion_labels.values())
            ax1.set_title(f'å¢å¼ºæ¨¡å‹\\nå‡†ç¡®ç‡: {enhanced_results["overall_accuracy"]:.4f}')
            ax1.set_xlabel('é¢„æµ‹æ ‡ç­¾')
            ax1.set_ylabel('çœŸå®æ ‡ç­¾')
            
            # åŸå§‹æ¨¡å‹
            cm2 = confusion_matrix(original_results['all_targets'], original_results['all_predictions'])
            sns.heatmap(cm2, annot=True, fmt='d', cmap='Reds', ax=ax2,
                       xticklabels=self.emotion_labels.values(),
                       yticklabels=self.emotion_labels.values())
            ax2.set_title(f'åŸå§‹æ¨¡å‹\\nå‡†ç¡®ç‡: {original_results["overall_accuracy"]:.4f}')
            ax2.set_xlabel('é¢„æµ‹æ ‡ç­¾')
            ax2.set_ylabel('çœŸå®æ ‡ç­¾')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.eval_dir, 'confusion_matrix_comparison.png'),
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_emotion_performance_comparison(self, enhanced_results, original_results):
        """ç»˜åˆ¶æƒ…æ„Ÿç±»åˆ«æ€§èƒ½å¯¹æ¯”"""
        # è®¡ç®—å„æƒ…æ„Ÿç±»åˆ«çš„å¹³å‡æ€§èƒ½
        enhanced_emotion_f1 = self._compute_emotion_f1_by_speaker(enhanced_results)
        
        fig, ax = plt.subplots(1, 1, figsize=(12, 8))
        
        emotion_names = list(self.emotion_labels.values())
        x = np.arange(len(emotion_names))
        width = 0.35
        
        # å¢å¼ºæ¨¡å‹
        enhanced_means = [np.mean(enhanced_emotion_f1[i]) for i in range(len(emotion_names))]
        enhanced_stds = [np.std(enhanced_emotion_f1[i]) for i in range(len(emotion_names))]
        
        bars1 = ax.bar(x - width/2, enhanced_means, width, yerr=enhanced_stds,
                      label='å¢å¼ºæ¨¡å‹', alpha=0.8, color=self.emotion_colors, capsize=5)
        
        if original_results:
            original_emotion_f1 = self._compute_emotion_f1_by_speaker(original_results)
            original_means = [np.mean(original_emotion_f1[i]) for i in range(len(emotion_names))]
            original_stds = [np.std(original_emotion_f1[i]) for i in range(len(emotion_names))]
            
            bars2 = ax.bar(x + width/2, original_means, width, yerr=original_stds,
                          label='åŸå§‹æ¨¡å‹', alpha=0.6, color=self.emotion_colors, capsize=5)
        
        ax.set_xlabel('æƒ…æ„Ÿç±»åˆ«')
        ax.set_ylabel('F1åˆ†æ•°')
        ax.set_title('å„æƒ…æ„Ÿç±»åˆ«è·¨è¯´è¯äººæ€§èƒ½å¯¹æ¯”')
        ax.set_xticks(x)
        ax.set_xticklabels(emotion_names)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, mean, std in zip(bars1, enhanced_means, enhanced_stds):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + std + 0.01,
                   f'{mean:.3f}', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.eval_dir, 'emotion_performance_comparison.png'),
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_speaker_bias_analysis(self, enhanced_results, original_results):
        """ç»˜åˆ¶è¯´è¯äººåè§åˆ†æ"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. è¯´è¯äººæ€§èƒ½æ–¹å·®åˆ†æ
        speakers = list(enhanced_results['speaker_results'].keys())
        enhanced_accs = [enhanced_results['speaker_results'][s]['accuracy'] for s in speakers]
        enhanced_f1s = [enhanced_results['speaker_results'][s]['f1_score'] for s in speakers]
        
        enhanced_acc_var = np.var(enhanced_accs)
        enhanced_f1_var = np.var(enhanced_f1s)
        
        metrics = ['å‡†ç¡®ç‡æ–¹å·®', 'F1åˆ†æ•°æ–¹å·®']
        enhanced_vars = [enhanced_acc_var, enhanced_f1_var]
        
        if original_results:
            original_accs = [original_results['speaker_results'][s]['accuracy'] for s in speakers]
            original_f1s = [original_results['speaker_results'][s]['f1_score'] for s in speakers]
            original_acc_var = np.var(original_accs)
            original_f1_var = np.var(original_f1s)
            original_vars = [original_acc_var, original_f1_var]
            
            x = np.arange(len(metrics))
            width = 0.35
            
            ax1.bar(x - width/2, enhanced_vars, width, label='å¢å¼ºæ¨¡å‹', alpha=0.8, color='skyblue')
            ax1.bar(x + width/2, original_vars, width, label='åŸå§‹æ¨¡å‹', alpha=0.8, color='lightcoral')
            ax1.set_xlabel('æŒ‡æ ‡')
            ax1.set_ylabel('æ–¹å·®')
            ax1.set_title('è¯´è¯äººé—´æ€§èƒ½æ–¹å·®å¯¹æ¯”\\n(æ–¹å·®è¶Šå°è¯´æ˜è·¨è¯´è¯äººæ³›åŒ–è¶Šå¥½)')
            ax1.set_xticks(x)
            ax1.set_xticklabels(metrics)
            ax1.legend()
            ax1.grid(True, alpha=0.3)
        else:
            ax1.bar(metrics, enhanced_vars, alpha=0.8, color=['skyblue', 'lightgreen'])
            ax1.set_xlabel('æŒ‡æ ‡')
            ax1.set_ylabel('æ–¹å·®')
            ax1.set_title('å¢å¼ºæ¨¡å‹è¯´è¯äººé—´æ€§èƒ½æ–¹å·®')
            ax1.grid(True, alpha=0.3)
        
        # 2. æ€§èƒ½åˆ†å¸ƒç®±çº¿å›¾
        if original_results:
            ax2.boxplot([enhanced_accs, original_accs], labels=['å¢å¼ºæ¨¡å‹', 'åŸå§‹æ¨¡å‹'])
            ax2.set_ylabel('å‡†ç¡®ç‡')
            ax2.set_title('è¯´è¯äººå‡†ç¡®ç‡åˆ†å¸ƒå¯¹æ¯”')
            ax2.grid(True, alpha=0.3)
        else:
            ax2.boxplot([enhanced_accs], labels=['å¢å¼ºæ¨¡å‹'])
            ax2.set_ylabel('å‡†ç¡®ç‡')
            ax2.set_title('å¢å¼ºæ¨¡å‹è¯´è¯äººå‡†ç¡®ç‡åˆ†å¸ƒ')
            ax2.grid(True, alpha=0.3)
        
        # 3. æ€§åˆ«å·®å¼‚åˆ†æ
        male_speakers = [s for s in speakers if 'M' in s]
        female_speakers = [s for s in speakers if 'F' in s]
        
        enhanced_male_acc = np.mean([enhanced_results['speaker_results'][s]['accuracy'] for s in male_speakers])
        enhanced_female_acc = np.mean([enhanced_results['speaker_results'][s]['accuracy'] for s in female_speakers])
        
        gender_labels = ['ç”·æ€§è¯´è¯äºº', 'å¥³æ€§è¯´è¯äºº']
        enhanced_gender_acc = [enhanced_male_acc, enhanced_female_acc]
        
        if original_results:
            original_male_acc = np.mean([original_results['speaker_results'][s]['accuracy'] for s in male_speakers])
            original_female_acc = np.mean([original_results['speaker_results'][s]['accuracy'] for s in female_speakers])
            original_gender_acc = [original_male_acc, original_female_acc]
            
            x = np.arange(len(gender_labels))
            width = 0.35
            
            ax3.bar(x - width/2, enhanced_gender_acc, width, label='å¢å¼ºæ¨¡å‹', alpha=0.8, color='skyblue')
            ax3.bar(x + width/2, original_gender_acc, width, label='åŸå§‹æ¨¡å‹', alpha=0.8, color='lightcoral')
        else:
            ax3.bar(gender_labels, enhanced_gender_acc, alpha=0.8, color=['lightblue', 'pink'])
        
        ax3.set_xlabel('è¯´è¯äººæ€§åˆ«')
        ax3.set_ylabel('å¹³å‡å‡†ç¡®ç‡')
        ax3.set_title('æ€§åˆ«å·®å¼‚åˆ†æ')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. ä¼šè¯å·®å¼‚åˆ†æ
        session_acc = defaultdict(list)
        for speaker in speakers:
            session = speaker[:7]  # æå–ä¼šè¯ä¿¡æ¯ (e.g., 'Ses01F' -> 'Session')
            session_acc[session].append(enhanced_results['speaker_results'][speaker]['accuracy'])
        
        sessions = list(session_acc.keys())
        session_means = [np.mean(session_acc[s]) for s in sessions]
        
        ax4.bar(sessions, session_means, alpha=0.8, color='lightgreen')
        ax4.set_xlabel('ä¼šè¯')
        ax4.set_ylabel('å¹³å‡å‡†ç¡®ç‡')
        ax4.set_title('ä¼šè¯é—´æ€§èƒ½å·®å¼‚')
        ax4.tick_params(axis='x', rotation=45)
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.eval_dir, 'speaker_bias_analysis.png'),
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _compute_emotion_f1_by_speaker(self, results):
        """è®¡ç®—å„è¯´è¯äººåœ¨æ¯ä¸ªæƒ…æ„Ÿç±»åˆ«ä¸Šçš„F1åˆ†æ•°"""
        emotion_f1_by_speaker = [[] for _ in range(4)]  # 4ä¸ªæƒ…æ„Ÿç±»åˆ«
        
        for speaker, speaker_result in results['speaker_results'].items():
            f1_per_class = speaker_result['f1_per_class']
            for i, f1 in enumerate(f1_per_class):
                emotion_f1_by_speaker[i].append(f1)
        
        return emotion_f1_by_speaker
    
    def save_results(self, comparison_results):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_path = os.path.join(self.eval_dir, 'detailed_results.json')
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(comparison_results, f, indent=2, ensure_ascii=False, default=str)
        
        # ç”Ÿæˆç®€åŒ–çš„å¯¹æ¯”æŠ¥å‘Š
        enhanced_results = comparison_results['enhanced']
        original_results = comparison_results['original']
        
        report = {
            'evaluation_time': datetime.now().isoformat(),
            'test_data_info': comparison_results['test_data_info'],
            'enhanced_model': {
                'overall_accuracy': enhanced_results['overall_accuracy'],
                'overall_f1': enhanced_results['overall_f1'],
                'speaker_performance_variance': {
                    'accuracy_var': np.var([r['accuracy'] for r in enhanced_results['speaker_results'].values()]),
                    'f1_var': np.var([r['f1_score'] for r in enhanced_results['speaker_results'].values()])
                }
            }
        }
        
        if original_results:
            report['original_model'] = {
                'overall_accuracy': original_results['overall_accuracy'],
                'overall_f1': original_results['overall_f1'],
                'speaker_performance_variance': {
                    'accuracy_var': np.var([r['accuracy'] for r in original_results['speaker_results'].values()]),
                    'f1_var': np.var([r['f1_score'] for r in original_results['speaker_results'].values()])
                }
            }
            
            # è®¡ç®—æ”¹è¿›å¹…åº¦
            report['improvement'] = {
                'accuracy_improvement': enhanced_results['overall_accuracy'] - original_results['overall_accuracy'],
                'f1_improvement': enhanced_results['overall_f1'] - original_results['overall_f1'],
                'variance_reduction': {
                    'accuracy': report['original_model']['speaker_performance_variance']['accuracy_var'] - 
                               report['enhanced_model']['speaker_performance_variance']['accuracy_var'],
                    'f1': report['original_model']['speaker_performance_variance']['f1_var'] - 
                          report['enhanced_model']['speaker_performance_variance']['f1_var']
                }
            }
        
        # ä¿å­˜ç®€åŒ–æŠ¥å‘Š
        report_path = os.path.join(self.eval_dir, 'comparison_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # æ‰“å°æ€»ç»“
        print("\\n" + "="*60)
        print("ğŸ“Š è·¨è¯´è¯äººæ€§èƒ½è¯„ä¼°æ€»ç»“")
        print("="*60)
        
        print(f"å¢å¼ºæ¨¡å‹æ€§èƒ½:")
        print(f"  æ€»ä½“å‡†ç¡®ç‡: {enhanced_results['overall_accuracy']:.4f}")
        print(f"  æ€»ä½“F1åˆ†æ•°: {enhanced_results['overall_f1']:.4f}")
        print(f"  è¯´è¯äººé—´å‡†ç¡®ç‡æ–¹å·®: {report['enhanced_model']['speaker_performance_variance']['accuracy_var']:.6f}")
        
        if original_results:
            print(f"\\nåŸå§‹æ¨¡å‹æ€§èƒ½:")
            print(f"  æ€»ä½“å‡†ç¡®ç‡: {original_results['overall_accuracy']:.4f}")
            print(f"  æ€»ä½“F1åˆ†æ•°: {original_results['overall_f1']:.4f}")
            print(f"  è¯´è¯äººé—´å‡†ç¡®ç‡æ–¹å·®: {report['original_model']['speaker_performance_variance']['accuracy_var']:.6f}")
            
            print(f"\\næ”¹è¿›æ•ˆæœ:")
            print(f"  å‡†ç¡®ç‡æå‡: {report['improvement']['accuracy_improvement']:.4f}")
            print(f"  F1åˆ†æ•°æå‡: {report['improvement']['f1_improvement']:.4f}")
            print(f"  æ–¹å·®å‡å°‘: {report['improvement']['variance_reduction']['accuracy']:.6f}")
        
        print(f"\\nğŸ“ è¯¦ç»†ç»“æœä¿å­˜åœ¨: {self.eval_dir}")

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="è·¨è¯´è¯äººæ€§èƒ½è¯„ä¼°")
    
    parser.add_argument('--enhanced_model_path', type=str, required=True,
                       help='å¢å¼ºæ¨¡å‹è·¯å¾„')
    parser.add_argument('--original_model_path', type=str, default=None,
                       help='åŸå§‹æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--data_path', type=str, default='./Train_data_org.pickle',
                       help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--cuda', action='store_true', default=True,
                       help='æ˜¯å¦ä½¿ç”¨CUDA')
    
    # æ¨¡å‹å‚æ•°ï¼ˆéœ€è¦ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    parser.add_argument('--hidden_layer', type=int, default=128,
                       help='éšè—å±‚å¤§å°')
    parser.add_argument('--out_class', type=int, default=4,
                       help='è¾“å‡ºç±»åˆ«æ•°')
    parser.add_argument('--dia_layers', type=int, default=2,
                       help='GRUå±‚æ•°')
    parser.add_argument('--dropout', type=float, default=0.3,
                       help='Dropoutæ¯”ä¾‹')
    parser.add_argument('--attention', action='store_true', default=True,
                       help='æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶')
    parser.add_argument('--speaker_norm', action='store_true', default=True,
                       help='æ˜¯å¦ä½¿ç”¨è¯´è¯äººå½’ä¸€åŒ–')
    parser.add_argument('--speaker_adversarial', action='store_true', default=True,
                       help='æ˜¯å¦ä½¿ç”¨è¯´è¯äººå¯¹æŠ—è®­ç»ƒ')
    parser.add_argument('--freeze_layers', type=int, default=6,
                       help='å†»ç»“HuBERTçš„å±‚æ•°')
    
    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¯åŠ¨è·¨è¯´è¯äººæ€§èƒ½è¯„ä¼°ç³»ç»Ÿ")
    print("="*60)
    
    args = parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = SpeakerIndependenceEvaluator(args)
    
    # è¿è¡Œè¯„ä¼°
    try:
        results = evaluator.compare_models(
            enhanced_model_path=args.enhanced_model_path,
            original_model_path=args.original_model_path
        )
        
        print("\\nğŸ‰ è¯„ä¼°å®Œæˆ!")
        
    except Exception as e:
        print(f"\\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()


