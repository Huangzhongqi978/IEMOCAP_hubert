#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºçš„GRUæ¨¡å‹æ¶æ„ - é’ˆå¯¹è·¨è¯´è¯äººæƒ…æ„Ÿè¯†åˆ«ä¼˜åŒ–
åŒ…å«è¯´è¯äººå½’ä¸€åŒ–ã€å¯¹æŠ—è®­ç»ƒã€å¢å¼ºç‰¹å¾æå–ç­‰åŠŸèƒ½
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.autograd import Function
import math

class GradientReversalLayer(Function):
    """æ¢¯åº¦åè½¬å±‚ - ç”¨äºè¯´è¯äººå¯¹æŠ—è®­ç»ƒ"""
    
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)
    
    @staticmethod
    def backward(ctx, grad_output):
        output = grad_output.neg() * ctx.alpha
        return output, None

def gradient_reverse(x, alpha=1.0):
    """æ¢¯åº¦åè½¬å‡½æ•°"""
    return GradientReversalLayer.apply(x, alpha)

class AdaptiveInstanceNormalization(nn.Module):
    """è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ– - è¯´è¯äººå½’ä¸€åŒ–å±‚"""
    
    def __init__(self, num_features, eps=1e-5):
        super(AdaptiveInstanceNormalization, self).__init__()
        self.num_features = num_features
        self.eps = eps
        
        # å¯å­¦ä¹ çš„ç¼©æ”¾å’Œåç§»å‚æ•°
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias = nn.Parameter(torch.zeros(num_features))
        
    def forward(self, x):
        """
        x: [batch_size, seq_len, num_features]
        """
        # è®¡ç®—å®ä¾‹çº§åˆ«çš„å‡å€¼å’Œæ–¹å·®ï¼ˆè·¨åºåˆ—ç»´åº¦ï¼‰
        mean = x.mean(dim=1, keepdim=True)  # [batch_size, 1, num_features]
        var = x.var(dim=1, keepdim=True, unbiased=False)  # [batch_size, 1, num_features]
        
        # å½’ä¸€åŒ–
        x_norm = (x - mean) / torch.sqrt(var + self.eps)
        
        # åº”ç”¨å¯å­¦ä¹ çš„ä»¿å°„å˜æ¢
        out = x_norm * self.weight.unsqueeze(0).unsqueeze(0) + self.bias.unsqueeze(0).unsqueeze(0)
        
        return out

class MultiHeadSelfAttention(nn.Module):
    """å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, d_model, num_heads, dropout=0.1):
        super(MultiHeadSelfAttention, self).__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model)
        
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
            
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        context = torch.matmul(attention_weights, V)
        return context, attention_weights
    
    def forward(self, x, mask=None):
        """
        x: [batch_size, seq_len, d_model]
        """
        batch_size, seq_len, d_model = x.size()
        residual = x
        
        # çº¿æ€§å˜æ¢å¾—åˆ°Q, K, V
        Q = self.w_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.w_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = self.w_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        
        # åº”ç”¨æ³¨æ„åŠ›
        context, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # æ‹¼æ¥å¤šå¤´
        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)
        
        # è¾“å‡ºæŠ•å½±
        output = self.w_o(context)
        
        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        output = self.layer_norm(output + residual)
        
        return output, attention_weights

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """
    
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        """
        x: [batch_size, seq_len, d_model]
        """
        seq_len = x.size(1)
        pos_encoding = self.pe[:seq_len, :].transpose(0, 1)  # [1, seq_len, d_model]
        return x + pos_encoding

class EnhancedGRUModel(nn.Module):
    """å¢å¼ºçš„GRUæ¨¡å‹ - é’ˆå¯¹è·¨è¯´è¯äººæƒ…æ„Ÿè¯†åˆ«ä¼˜åŒ–"""
    
    def __init__(self, input_size, hidden_size, output_size, args):
        super(EnhancedGRUModel, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.num_layers = args.dia_layers
        self.dropout_rate = args.dropout
        self.use_attention = args.attention
        self.use_speaker_norm = getattr(args, 'speaker_norm', True)
        self.use_adversarial = getattr(args, 'speaker_adversarial', True)
        
        # è¾“å…¥æŠ•å½±å±‚
        self.input_projection = nn.Linear(input_size, hidden_size * 2)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(hidden_size * 2)
        
        # è¯´è¯äººå½’ä¸€åŒ–å±‚
        if self.use_speaker_norm:
            self.speaker_norm = AdaptiveInstanceNormalization(hidden_size * 2)
        
        # åŒå‘GRUå±‚
        self.gru_layers = nn.ModuleList()
        self.layer_norms = nn.ModuleList()
        
        for i in range(self.num_layers):
            input_dim = hidden_size * 2 if i == 0 else hidden_size * 4
            self.gru_layers.append(
                nn.GRU(input_dim, hidden_size * 2, batch_first=True, 
                      bidirectional=True, dropout=self.dropout_rate if i < self.num_layers-1 else 0)
            )
            self.layer_norms.append(nn.LayerNorm(hidden_size * 4))
        
        # å¤šå¤´è‡ªæ³¨æ„åŠ›
        if self.use_attention:
            self.self_attention = MultiHeadSelfAttention(
                d_model=hidden_size * 4, 
                num_heads=8, 
                dropout=self.dropout_rate
            )
        
        # ç‰¹å¾å¢å¼ºæ¨¡å—
        self.feature_enhancement = nn.Sequential(
            nn.Linear(hidden_size * 4, hidden_size * 2),
            nn.ReLU(inplace=True),
            nn.Dropout(self.dropout_rate),
            nn.Linear(hidden_size * 2, hidden_size * 2),
            nn.ReLU(inplace=True),
            nn.Dropout(self.dropout_rate)
        )
        
        # å…¨å±€æ± åŒ–ç­–ç•¥
        self.global_pooling = nn.AdaptiveAvgPool1d(1)
        self.global_max_pooling = nn.AdaptiveMaxPool1d(1)
        
        # æƒ…æ„Ÿåˆ†ç±»å¤´
        self.emotion_classifier = nn.Sequential(
            nn.Linear(hidden_size * 4, hidden_size),  # avg + max pooling
            nn.ReLU(inplace=True),
            nn.Dropout(self.dropout_rate),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(self.dropout_rate),
            nn.Linear(hidden_size // 2, output_size)
        )
        
        # è¯´è¯äººåˆ†ç±»å¤´ï¼ˆç”¨äºå¯¹æŠ—è®­ç»ƒï¼‰
        if self.use_adversarial:
            self.speaker_classifier = nn.Sequential(
                nn.Linear(hidden_size * 4, hidden_size),
                nn.ReLU(inplace=True),
                nn.Dropout(self.dropout_rate),
                nn.Linear(hidden_size, hidden_size // 2),
                nn.ReLU(inplace=True),
                nn.Dropout(self.dropout_rate),
                nn.Linear(hidden_size // 2, 10)  # IEMOCAPæœ‰10ä¸ªè¯´è¯äºº
            )
        
        self.dropout = nn.Dropout(self.dropout_rate)
        self._init_weights()
        
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for name, param in self.named_parameters():
            if 'weight' in name:
                if len(param.shape) >= 2:
                    nn.init.xavier_uniform_(param)
                else:
                    nn.init.uniform_(param, -0.1, 0.1)
            elif 'bias' in name:
                nn.init.zeros_(param)
    
    def forward(self, x, alpha=1.0):
        """
        å‰å‘ä¼ æ’­
        x: [batch_size, seq_len, input_size]
        alpha: æ¢¯åº¦åè½¬å¼ºåº¦ï¼ˆç”¨äºå¯¹æŠ—è®­ç»ƒï¼‰
        """
        batch_size, seq_len, _ = x.size()
        
        # è¾“å…¥æŠ•å½±å’Œä½ç½®ç¼–ç 
        x = self.input_projection(x)  # [batch_size, seq_len, hidden_size*2]
        x = self.pos_encoding(x)
        
        # è¯´è¯äººå½’ä¸€åŒ–
        if self.use_speaker_norm:
            x = self.speaker_norm(x)
        
        x = self.dropout(x)
        
        # å¤šå±‚åŒå‘GRU
        for i, (gru_layer, layer_norm) in enumerate(zip(self.gru_layers, self.layer_norms)):
            residual = x if i > 0 else None
            
            gru_out, _ = gru_layer(x)  # [batch_size, seq_len, hidden_size*4]
            gru_out = layer_norm(gru_out)
            
            # æ®‹å·®è¿æ¥ï¼ˆä»ç¬¬äºŒå±‚å¼€å§‹ï¼‰
            if residual is not None and residual.size(-1) == gru_out.size(-1):
                gru_out = gru_out + residual
            
            x = self.dropout(gru_out)
        
        # å¤šå¤´è‡ªæ³¨æ„åŠ›
        attention_weights = None
        if self.use_attention:
            x, attention_weights = self.self_attention(x)
        
        # ç‰¹å¾å¢å¼º
        enhanced_features = self.feature_enhancement(x)  # [batch_size, seq_len, hidden_size*2]
        
        # æ‹¼æ¥åŸå§‹ç‰¹å¾å’Œå¢å¼ºç‰¹å¾
        combined_features = torch.cat([x, enhanced_features], dim=-1)  # [batch_size, seq_len, hidden_size*6]
        
        # é™ç»´åˆ°ç»Ÿä¸€å¤§å°
        combined_features = combined_features[:, :, :self.hidden_size*4]  # [batch_size, seq_len, hidden_size*4]
        
        # å…¨å±€æ± åŒ–
        # è½¬ç½®ç”¨äºæ± åŒ–æ“ä½œ
        pooling_input = combined_features.transpose(1, 2)  # [batch_size, hidden_size*4, seq_len]
        
        avg_pooled = self.global_pooling(pooling_input).squeeze(-1)  # [batch_size, hidden_size*4]
        max_pooled = self.global_max_pooling(pooling_input).squeeze(-1)  # [batch_size, hidden_size*4]
        
        # æ‹¼æ¥å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–ç»“æœ
        global_features = torch.cat([avg_pooled, max_pooled], dim=-1)  # [batch_size, hidden_size*8]
        
        # é™ç»´åˆ°ç»Ÿä¸€å¤§å°
        global_features = global_features[:, :self.hidden_size*4]  # [batch_size, hidden_size*4]
        
        # æƒ…æ„Ÿåˆ†ç±»
        emotion_logits = self.emotion_classifier(global_features)
        
        # è¯´è¯äººå¯¹æŠ—åˆ†ç±»
        speaker_logits = None
        if self.use_adversarial:
            reversed_features = gradient_reverse(global_features, alpha)
            speaker_logits = self.speaker_classifier(reversed_features)
        
        return {
            'emotion_logits': emotion_logits,
            'speaker_logits': speaker_logits,
            'attention_weights': attention_weights,
            'global_features': global_features
        }

class EnhancedSpeechRecognitionModel(nn.Module):
    """å¢å¼ºçš„è¯­éŸ³è¯†åˆ«æ¨¡å‹ä¸»ç±»"""
    
    def __init__(self, args):
        super(EnhancedSpeechRecognitionModel, self).__init__()
        
        # HuBERTç‰¹å¾æå–å™¨ï¼ˆå†»ç»“éƒ¨åˆ†å±‚ä»¥å‡å°‘è¿‡æ‹Ÿåˆï¼‰
        from transformers import HubertModel
        self.feature_extractor = HubertModel.from_pretrained("facebook/hubert-base-ls960")
        
        # å†»ç»“å‰å‡ å±‚
        freeze_layers = getattr(args, 'freeze_layers', 6)
        if freeze_layers > 0:
            for i, layer in enumerate(self.feature_extractor.encoder.layers):
                if i < freeze_layers:
                    for param in layer.parameters():
                        param.requires_grad = False
        
        # ç‰¹å¾ç»´åº¦é€‚é…
        hubert_dim = self.feature_extractor.config.hidden_size  # 768
        
        # å¢å¼ºçš„GRUæ¨¡å‹
        self.utterance_net = EnhancedGRUModel(
            input_size=hubert_dim,
            hidden_size=args.hidden_layer,
            output_size=args.out_class,
            args=args
        )
        
        self.dropout = nn.Dropout(args.dropout)
        
    def forward(self, input_waveform, alpha=1.0):
        """
        å‰å‘ä¼ æ’­
        input_waveform: éŸ³é¢‘æ³¢å½¢è¾“å…¥
        alpha: å¯¹æŠ—è®­ç»ƒå¼ºåº¦
        """
        # HuBERTç‰¹å¾æå–
        with torch.no_grad() if hasattr(self, '_freeze_hubert') else torch.enable_grad():
            hubert_features = self.feature_extractor(input_waveform).last_hidden_state
        
        hubert_features = self.dropout(hubert_features)
        
        # é€šè¿‡å¢å¼ºGRUç½‘ç»œ
        outputs = self.utterance_net(hubert_features, alpha=alpha)
        
        return outputs

def create_enhanced_model(args):
    """åˆ›å»ºå¢å¼ºæ¨¡å‹çš„å·¥å‚å‡½æ•°"""
    
    # è®¾ç½®é»˜è®¤å‚æ•°
    if not hasattr(args, 'speaker_norm'):
        args.speaker_norm = True
    if not hasattr(args, 'speaker_adversarial'):
        args.speaker_adversarial = True
    if not hasattr(args, 'freeze_layers'):
        args.freeze_layers = 6
    
    model = EnhancedSpeechRecognitionModel(args)
    
    print("ğŸš€ å¢å¼ºæ¨¡å‹åˆ›å»ºæˆåŠŸ!")
    print(f"   - è¯´è¯äººå½’ä¸€åŒ–: {'âœ“' if args.speaker_norm else 'âœ—'}")
    print(f"   - å¯¹æŠ—è®­ç»ƒ: {'âœ“' if args.speaker_adversarial else 'âœ—'}")
    print(f"   - å†»ç»“HuBERTå±‚æ•°: {args.freeze_layers}")
    print(f"   - æ³¨æ„åŠ›æœºåˆ¶: {'âœ“' if args.attention else 'âœ—'}")
    
    return model

if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹
    class Args:
        def __init__(self):
            self.hidden_layer = 128
            self.out_class = 4
            self.dia_layers = 2
            self.dropout = 0.3
            self.attention = True
            self.speaker_norm = True
            self.speaker_adversarial = True
            self.freeze_layers = 6
    
    args = Args()
    model = create_enhanced_model(args)
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size, seq_len, feature_dim = 4, 100, 768
    dummy_input = torch.randn(batch_size, seq_len, feature_dim)
    
    outputs = model.utterance_net(dummy_input)
    
    print(f"è¾“å…¥å½¢çŠ¶: {dummy_input.shape}")
    print(f"æƒ…æ„Ÿåˆ†ç±»è¾“å‡ºå½¢çŠ¶: {outputs['emotion_logits'].shape}")
    if outputs['speaker_logits'] is not None:
        print(f"è¯´è¯äººåˆ†ç±»è¾“å‡ºå½¢çŠ¶: {outputs['speaker_logits'].shape}")
    print("âœ… æ¨¡å‹æµ‹è¯•é€šè¿‡!")


