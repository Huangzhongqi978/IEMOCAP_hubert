#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿæ¨¡å‹è¯„ä¼°å·¥å…·
ç”¨äºå¿«é€Ÿæµ‹è¯•å·²è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix
import argparse
from models import SpeechRecognitionModel
from utils import Get_data
import pickle

# æƒ…æ„Ÿæ ‡ç­¾æ˜ å°„
EMOTION_LABELS = {0: 'Angry', 1: 'Happy', 2: 'Neutral', 3: 'Sad'}
EMOTION_COLORS = ['#e74c3c', '#f39c12', '#95a5a6', '#3498db']

def get_default_args():
    """è·å–é»˜è®¤å‚æ•°é…ç½®"""
    parser = argparse.ArgumentParser()
    parser.add_argument('--cuda', action='store_false')
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--dropout', type=float, default=0.2)
    parser.add_argument('--attention', action='store_true', default=True)
    parser.add_argument('--dia_layers', type=int, default=2)
    parser.add_argument('--hidden_layer', type=int, default=256)
    parser.add_argument('--out_class', type=int, default=4)
    parser.add_argument('--utt_insize', type=int, default=768)
    args, _ = parser.parse_known_args([])
    return args

def load_model_and_data(model_path='model.pkl', data_path='Train_data_org.pickle'):
    """åŠ è½½æ¨¡å‹å’Œæ•°æ®"""
    print("ğŸ“¥ åŠ è½½æ¨¡å‹å’Œæ•°æ®...")
    
    # åŠ è½½æ•°æ®
    try:
        with open(data_path, 'rb') as file:
            data = pickle.load(file)
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(data)} ä¸ªæ ·æœ¬")
    except FileNotFoundError:
        print(f"âŒ æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {data_path}")
        return None, None
    
    # åŠ è½½æ¨¡å‹
    args = get_default_args()
    model = SpeechRecognitionModel(args)
    
    try:
        state_dict = torch.load(model_path, map_location='cpu')
        model.load_state_dict(state_dict)
        model.eval()
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None
    
    return model, data

def evaluate_model_simple(model, data, test_ratio=0.2):
    """ç®€å•è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print("ğŸ” å¼€å§‹æ¨¡å‹è¯„ä¼°...")
    
    # ç®€å•åˆ’åˆ†æ•°æ®é›†
    np.random.seed(42)
    indices = np.random.permutation(len(data))
    test_size = int(len(data) * test_ratio)
    test_indices = indices[:test_size]
    train_indices = indices[test_size:]
    
    # è·å–æµ‹è¯•æ•°æ®åŠ è½½å™¨
    args = get_default_args()
    _, test_loader, _, _ = Get_data(data, train_indices, test_indices, args)
    
    # è¯„ä¼°æ¨¡å‹
    predictions = []
    true_labels = []
    probabilities = []
    
    with torch.no_grad():
        for batch_idx, (data_batch, target) in enumerate(test_loader):
            data_batch = data_batch.squeeze()
            target = target.squeeze()
            
            # å‰å‘ä¼ æ’­
            output, _ = model(data_batch)
            
            # è·å–é¢„æµ‹
            pred = torch.argmax(output, dim=1)
            probs = torch.softmax(output, dim=1)
            
            predictions.extend(pred.cpu().numpy())
            true_labels.extend(target.cpu().numpy())
            probabilities.extend(probs.cpu().numpy())
    
    return np.array(true_labels), np.array(predictions), np.array(probabilities)

def calculate_metrics(y_true, y_pred):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    # åŸºæœ¬æŒ‡æ ‡
    accuracy = accuracy_score(y_true, y_pred)
    f1_macro = f1_score(y_true, y_pred, average='macro')
    recall_macro = recall_score(y_true, y_pred, average='macro')
    precision_macro = precision_score(y_true, y_pred, average='macro')
    
    # æ¯ç±»æŒ‡æ ‡
    f1_per_class = f1_score(y_true, y_pred, average=None)
    recall_per_class = recall_score(y_true, y_pred, average=None)
    precision_per_class = precision_score(y_true, y_pred, average=None)
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    
    # UA å’Œ WA
    wa = accuracy
    ua = np.mean(np.diag(cm) / np.sum(cm, axis=1))
    
    return {
        'accuracy': accuracy,
        'f1_macro': f1_macro,
        'recall_macro': recall_macro,
        'precision_macro': precision_macro,
        'f1_per_class': f1_per_class,
        'recall_per_class': recall_per_class,
        'precision_per_class': precision_per_class,
        'confusion_matrix': cm,
        'ua': ua,
        'wa': wa
    }

def print_results(metrics):
    """æ‰“å°è¯„ä¼°ç»“æœ"""
    print("\n" + "="*50)
    print("ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ")
    print("="*50)
    
    print(f"\nğŸ¯ æ€»ä½“æ€§èƒ½:")
    print(f"   å‡†ç¡®ç‡ (Accuracy): {metrics['accuracy']:.4f}")
    print(f"   F1åˆ†æ•° (Macro):   {metrics['f1_macro']:.4f}")
    print(f"   å¬å›ç‡ (Macro):   {metrics['recall_macro']:.4f}")
    print(f"   ç²¾ç¡®ç‡ (Macro):   {metrics['precision_macro']:.4f}")
    print(f"   UA (æ— æƒé‡å‡†ç¡®ç‡): {metrics['ua']:.4f}")
    print(f"   WA (åŠ æƒå‡†ç¡®ç‡):   {metrics['wa']:.4f}")
    
    print(f"\nğŸ­ å„æƒ…æ„Ÿç±»åˆ«æ€§èƒ½:")
    for i, emotion in EMOTION_LABELS.items():
        print(f"   {emotion:8s}: F1={metrics['f1_per_class'][i]:.4f}, "
              f"Precision={metrics['precision_per_class'][i]:.4f}, "
              f"Recall={metrics['recall_per_class'][i]:.4f}")

def create_simple_visualization(metrics, save_plot=True):
    """åˆ›å»ºç®€å•çš„å¯è§†åŒ–å›¾è¡¨"""
    print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. æ··æ·†çŸ©é˜µ
    cm = metrics['confusion_matrix']
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',
               xticklabels=list(EMOTION_LABELS.values()),
               yticklabels=list(EMOTION_LABELS.values()),
               ax=ax1)
    ax1.set_title('æ··æ·†çŸ©é˜µ (å½’ä¸€åŒ–)', fontsize=14, fontweight='bold')
    ax1.set_xlabel('é¢„æµ‹æ ‡ç­¾')
    ax1.set_ylabel('çœŸå®æ ‡ç­¾')
    
    # 2. å„ç±»åˆ«æ€§èƒ½å¯¹æ¯”
    emotions = list(EMOTION_LABELS.values())
    precision = metrics['precision_per_class']
    recall = metrics['recall_per_class']
    f1 = metrics['f1_per_class']
    
    x = np.arange(len(emotions))
    width = 0.25
    
    ax2.bar(x - width, precision, width, label='ç²¾ç¡®ç‡', color='#3498db', alpha=0.8)
    ax2.bar(x, recall, width, label='å¬å›ç‡', color='#e74c3c', alpha=0.8)
    ax2.bar(x + width, f1, width, label='F1åˆ†æ•°', color='#2ecc71', alpha=0.8)
    
    ax2.set_xlabel('æƒ…æ„Ÿç±»åˆ«')
    ax2.set_ylabel('æ€§èƒ½æŒ‡æ ‡')
    ax2.set_title('å„æƒ…æ„Ÿç±»åˆ«æ€§èƒ½å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax2.set_xticks(x)
    ax2.set_xticklabels(emotions)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. æ€»ä½“æŒ‡æ ‡å¯¹æ¯”
    metrics_names = ['å‡†ç¡®ç‡', 'F1åˆ†æ•°', 'å¬å›ç‡', 'ç²¾ç¡®ç‡']
    metrics_values = [metrics['accuracy'], metrics['f1_macro'], 
                     metrics['recall_macro'], metrics['precision_macro']]
    
    bars = ax3.bar(metrics_names, metrics_values, 
                   color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'], alpha=0.8)
    ax3.set_title('æ€»ä½“æ€§èƒ½æŒ‡æ ‡', fontsize=14, fontweight='bold')
    ax3.set_ylabel('åˆ†æ•°')
    ax3.set_ylim(0, 1)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, metrics_values):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # 4. æ··æ·†çŸ©é˜µåŸå§‹æ•°å€¼
    sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges',
               xticklabels=list(EMOTION_LABELS.values()),
               yticklabels=list(EMOTION_LABELS.values()),
               ax=ax4)
    ax4.set_title('æ··æ·†çŸ©é˜µ (åŸå§‹æ•°å€¼)', fontsize=14, fontweight='bold')
    ax4.set_xlabel('é¢„æµ‹æ ‡ç­¾')
    ax4.set_ylabel('çœŸå®æ ‡ç­¾')
    
    plt.tight_layout()
    
    if save_plot:
        plt.savefig('quick_evaluation_report.png', dpi=300, bbox_inches='tight')
        print("ğŸ“Š å¯è§†åŒ–æŠ¥å‘Šå·²ä¿å­˜è‡³: quick_evaluation_report.png")
    
    plt.show()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ IEMOCAP è¯­éŸ³æƒ…æ„Ÿè¯†åˆ« - å¿«é€Ÿè¯„ä¼°å·¥å…·")
    print("=" * 50)
    
    # åŠ è½½æ¨¡å‹å’Œæ•°æ®
    model, data = load_model_and_data()
    
    if model is None or data is None:
        print("âŒ åŠ è½½å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
        return
    
    # è¯„ä¼°æ¨¡å‹
    y_true, y_pred, y_probs = evaluate_model_simple(model, data)
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = calculate_metrics(y_true, y_pred)
    
    # æ‰“å°ç»“æœ
    print_results(metrics)
    
    # åˆ›å»ºå¯è§†åŒ–
    create_simple_visualization(metrics, save_plot=True)
    
    print("\nâœ… å¿«é€Ÿè¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°: {len(y_true)}")
    print(f"ğŸ¯ ä¸»è¦æŒ‡æ ‡: å‡†ç¡®ç‡={metrics['accuracy']:.4f}, F1åˆ†æ•°={metrics['f1_macro']:.4f}")

if __name__ == "__main__":
    main()


