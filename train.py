import argparse
import pickle
import copy
import torch
import torch.optim as optim
from utils import Get_data
from torch.autograd import Variable
from models import SpeechRecognitionModel
from sklearn.metrics import confusion_matrix, classification_report
from sklearn import metrics
from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import os
from datetime import datetime
import json
from matplotlib.font_manager import FontProperties
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans', 'Arial Unicode MS']
matplotlib.rcParams['axes.unicode_minus'] = False
matplotlib.rcParams['figure.dpi'] = 100
matplotlib.rcParams['savefig.dpi'] = 300

# è§£å†³ä¸­æ–‡ç¼–ç é—®é¢˜
import locale
try:
    locale.setlocale(locale.LC_ALL, 'zh_CN.UTF-8')
except:
    try:
        locale.setlocale(locale.LC_ALL, 'Chinese_China.UTF-8')
    except:
        print("âš ï¸ æ— æ³•è®¾ç½®ä¸­æ–‡æœ¬åœ°åŒ–ï¼Œå¯èƒ½å½±å“ä¸­æ–‡æ˜¾ç¤º")



with open('./Train_data_org.pickle', 'rb') as file:
    data = pickle.load(file)

parser = argparse.ArgumentParser(description="RNN_Model")
parser.add_argument('--cuda', action='store_false')
parser.add_argument('--bid_flag', action='store_false')
parser.add_argument('--batch_first', action='store_false')
parser.add_argument('--batch_size', type=int, default=32, metavar='N')
parser.add_argument('--log_interval', type=int, default=10, metavar='N')
parser.add_argument('--dropout', type=float, default=0.2)
parser.add_argument('--epochs', type=int, default=20)
parser.add_argument('--lr', type=float, default=1e-5)
parser.add_argument('--optim', type=str, default='AdamW')
parser.add_argument('--attention', action='store_true', default=True)
parser.add_argument('--seed', type=int, default=1111)
parser.add_argument('--dia_layers', type=int, default=2)
parser.add_argument('--hidden_layer', type=int, default=256)
parser.add_argument('--out_class', type=int, default=4)
parser.add_argument('--utt_insize', type=int, default=768)
parser.add_argument('--save_dir', type=str, default='./results', help='Directory to save results')
parser.add_argument('--exp_name', type=str, default='emotion_recognition', help='Experiment name')
args = parser.parse_args()

torch.manual_seed(args.seed)

# åˆ›å»ºç»“æœä¿å­˜ç›®å½•
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
exp_dir = os.path.join(args.save_dir, f"{args.exp_name}_{timestamp}")
os.makedirs(exp_dir, exist_ok=True)
os.makedirs(os.path.join(exp_dir, 'plots'), exist_ok=True)
os.makedirs(os.path.join(exp_dir, 'logs'), exist_ok=True)

# æƒ…æ„Ÿæ ‡ç­¾æ˜ å°„
emotion_labels = {0: 'Angry', 1: 'Happy', 2: 'Neutral', 3: 'Sad'}
label_names = list(emotion_labels.values())
emotion_colors = ['#e74c3c', '#f1c40f', '#95a5a6', '#3498db']  # å¯¹åº”æ„¤æ€’ã€é«˜å…´ã€ä¸­æ€§ã€æ‚²ä¼¤çš„é¢œè‰²

print(f"ğŸš€ å®éªŒå¼€å§‹: {args.exp_name}")
print(f"ğŸ“ ç»“æœä¿å­˜ç›®å½•: {exp_dir}")
print(f"âš™ï¸ å®éªŒå‚æ•°: {vars(args)}")

# ä¿å­˜å®éªŒé…ç½®
config_path = os.path.join(exp_dir, 'config.json')
with open(config_path, 'w', encoding='utf-8') as f:
    json.dump(vars(args), f, indent=2, ensure_ascii=False)


def Train(epoch):
    train_loss = 0
    model.train()
    epoch_losses = []
    
    for batch_idx, (data_1, target) in enumerate(train_loader):
        if args.cuda:
            data_1, target = data_1.cuda(), target.cuda()
        data_1, target = Variable(data_1), Variable(target)
        target = target.squeeze()
        utt_optim.zero_grad()
        data_1 = data_1.squeeze()
        utt_out, _ = model(data_1)
        loss = torch.nn.CrossEntropyLoss()(utt_out, target.long())

        loss.backward()

        utt_optim.step()
        train_loss += loss
        epoch_losses.append(loss.item())

        if batch_idx > 0 and batch_idx % args.log_interval == 0:
            avg_loss = train_loss.item() / args.log_interval
            print('ğŸ“ˆ Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * args.batch_size, len(train_loader.dataset),
                       100. * batch_idx / len(train_loader), avg_loss
            ))
            train_loss = 0
    
    return np.mean(epoch_losses)

def Test():
    model.eval()
    label_pre = []
    label_true = []
    fea_pre = []
    test_loss = 0
    
    with torch.no_grad():
        for batch_idx, (data_1, target) in enumerate(test_loader):
            if args.cuda:
                data_1, target = data_1.cuda(), target.cuda()
            data_1, target = Variable(data_1), Variable(target)
            target = target.squeeze(1)
            data_1 = data_1.squeeze(1)
            data_1 = data_1.squeeze(1)
            utt_out, hid = model(data_1)
            
            # è®¡ç®—æµ‹è¯•æŸå¤±
            loss = torch.nn.CrossEntropyLoss()(utt_out, target.long())
            test_loss += loss.item()
            
            output = torch.argmax(utt_out, dim=1)
            fea_pre.extend(utt_out.cpu().data.numpy())
            label_true.extend(target.cpu().data.numpy())
            label_pre.extend(output.cpu().data.numpy())
    
    # è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
    accuracy = accuracy_score(label_true, label_pre)
    precision_macro = precision_score(label_true, label_pre, average='macro')
    precision_weighted = precision_score(label_true, label_pre, average='weighted')
    recall_macro = recall_score(label_true, label_pre, average='macro')
    recall_weighted = recall_score(label_true, label_pre, average='weighted')
    f1_macro = f1_score(label_true, label_pre, average='macro')
    f1_weighted = f1_score(label_true, label_pre, average='weighted')
    
    # æ¯ç±»åˆ«æŒ‡æ ‡
    precision_per_class = precision_score(label_true, label_pre, average=None)
    recall_per_class = recall_score(label_true, label_pre, average=None)
    f1_per_class = f1_score(label_true, label_pre, average=None)
    
    # UA (Unweighted Accuracy) = Macro Recall
    # WA (Weighted Accuracy) = Overall Accuracy
    ua = recall_macro
    wa = accuracy
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(label_true, label_pre)
    
    # åˆ†ç±»æŠ¥å‘Š
    class_report = classification_report(label_true, label_pre, 
                                       target_names=label_names, 
                                       output_dict=True)
    
    # è¯¦ç»†è¾“å‡ºç»“æœ
    print("="*80)
    print("ğŸ“Š è¯¦ç»†è¯„ä¼°ç»“æœ")
    print("="*80)
    print(f"ğŸ¯ æ•´ä½“å‡†ç¡®ç‡ (Overall Accuracy): {accuracy:.4f}")
    print(f"ğŸ¯ UA (Unweighted Accuracy): {ua:.4f}")
    print(f"ğŸ¯ WA (Weighted Accuracy): {wa:.4f}")
    print(f"ğŸ“ˆ æµ‹è¯•æŸå¤± (Test Loss): {test_loss/len(test_loader):.4f}")
    print()
    
    print("ğŸ“ˆ å®å¹³å‡æŒ‡æ ‡ (Macro Average):")
    print(f"  ç²¾ç¡®ç‡ (Precision): {precision_macro:.4f}")
    print(f"  å¬å›ç‡ (Recall): {recall_macro:.4f}")
    print(f"  F1åˆ†æ•° (F1-Score): {f1_macro:.4f}")
    print()
    
    print("ğŸ“ˆ åŠ æƒå¹³å‡æŒ‡æ ‡ (Weighted Average):")
    print(f"  ç²¾ç¡®ç‡ (Precision): {precision_weighted:.4f}")
    print(f"  å¬å›ç‡ (Recall): {recall_weighted:.4f}")
    print(f"  F1åˆ†æ•° (F1-Score): {f1_weighted:.4f}")
    print()
    
    print("ğŸ“ˆ å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡:")
    for i, (label, emotion) in enumerate(emotion_labels.items()):
        if i < len(precision_per_class):
            print(f"  {emotion:>8}: P={precision_per_class[i]:.4f}, R={recall_per_class[i]:.4f}, F1={f1_per_class[i]:.4f}")
    print()
    
    print("ğŸ” æ··æ·†çŸ©é˜µ:")
    print("    ", "  ".join([f"{name:>8}" for name in label_names]))
    for i, row in enumerate(cm):
        print(f"{label_names[i]:>8}", "  ".join([f"{val:>8}" for val in row]))
    print()
    
    # è¿”å›æ‰€æœ‰æŒ‡æ ‡
    metrics_dict = {
        'accuracy': accuracy,
        'precision_macro': precision_macro,
        'precision_weighted': precision_weighted,
        'recall_macro': recall_macro,
        'recall_weighted': recall_weighted,
        'f1_macro': f1_macro,
        'f1_weighted': f1_weighted,
        'ua': ua,
        'wa': wa,
        'test_loss': test_loss/len(test_loader),
        'confusion_matrix': cm,
        'classification_report': class_report,
        'per_class_precision': precision_per_class,
        'per_class_recall': recall_per_class,
        'per_class_f1': f1_per_class
    }
    
    return metrics_dict, label_pre, label_true, fea_pre

def plot_confusion_matrix(cm, fold_idx, epoch, save_path):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=label_names, yticklabels=label_names)
    plt.title(f'æ··æ·†çŸ©é˜µ - Fold {fold_idx+1}, Epoch {epoch}')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_training_curves(train_losses, test_losses, metrics_history, fold_idx, save_path):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    epochs = range(1, len(train_losses) + 1)
    
    fig, axes = plt.subplots(2, 3, figsize=(20, 12))
    
    # æŸå¤±æ›²çº¿
    axes[0, 0].plot(epochs, train_losses, 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2, marker='o', markersize=4)
    axes[0, 0].plot(epochs, test_losses, 'r-', label='æµ‹è¯•æŸå¤±', linewidth=2, marker='s', markersize=4)
    axes[0, 0].set_title(f'è®­ç»ƒ/æµ‹è¯•æŸå¤± - Fold {fold_idx+1}', fontsize=14, fontweight='bold')
    axes[0, 0].set_xlabel('è½®æ¬¡ (Epoch)', fontsize=12)
    axes[0, 0].set_ylabel('æŸå¤±å€¼ (Loss)', fontsize=12)
    axes[0, 0].legend(fontsize=11)
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].set_facecolor('#f8f9fa')
    
    # å‡†ç¡®ç‡æ›²çº¿
    accuracies = [m['accuracy'] for m in metrics_history]
    ua_scores = [m['ua'] for m in metrics_history]
    axes[0, 1].plot(epochs, accuracies, 'g-', label='æ•´ä½“å‡†ç¡®ç‡', linewidth=2, marker='o', markersize=4)
    axes[0, 1].plot(epochs, ua_scores, 'm-', label='æ— æƒé‡å‡†ç¡®ç‡(UA)', linewidth=2, marker='s', markersize=4)
    axes[0, 1].set_title(f'å‡†ç¡®ç‡æ›²çº¿ - Fold {fold_idx+1}', fontsize=14, fontweight='bold')
    axes[0, 1].set_xlabel('è½®æ¬¡ (Epoch)', fontsize=12)
    axes[0, 1].set_ylabel('å‡†ç¡®ç‡', fontsize=12)
    axes[0, 1].legend(fontsize=11)
    axes[0, 1].grid(True, alpha=0.3)
    axes[0, 1].set_facecolor('#f8f9fa')
    
    # F1åˆ†æ•°æ›²çº¿
    f1_macro = [m['f1_macro'] for m in metrics_history]
    f1_weighted = [m['f1_weighted'] for m in metrics_history]
    axes[0, 2].plot(epochs, f1_macro, 'orange', label='F1å®å¹³å‡', linewidth=2, marker='o', markersize=4)
    axes[0, 2].plot(epochs, f1_weighted, 'purple', label='F1åŠ æƒå¹³å‡', linewidth=2, marker='s', markersize=4)
    axes[0, 2].set_title(f'F1åˆ†æ•°æ›²çº¿ - Fold {fold_idx+1}', fontsize=14, fontweight='bold')
    axes[0, 2].set_xlabel('è½®æ¬¡ (Epoch)', fontsize=12)
    axes[0, 2].set_ylabel('F1åˆ†æ•°', fontsize=12)
    axes[0, 2].legend(fontsize=11)
    axes[0, 2].grid(True, alpha=0.3)
    axes[0, 2].set_facecolor('#f8f9fa')
    
    # ç²¾ç¡®ç‡å’Œå¬å›ç‡æ›²çº¿
    precision_macro = [m['precision_macro'] for m in metrics_history]
    recall_macro = [m['recall_macro'] for m in metrics_history]
    axes[1, 0].plot(epochs, precision_macro, 'cyan', label='ç²¾ç¡®ç‡(å®å¹³å‡)', linewidth=2, marker='o', markersize=4)
    axes[1, 0].plot(epochs, recall_macro, 'brown', label='å¬å›ç‡(å®å¹³å‡)', linewidth=2, marker='s', markersize=4)
    axes[1, 0].set_title(f'ç²¾ç¡®ç‡/å¬å›ç‡æ›²çº¿ - Fold {fold_idx+1}', fontsize=14, fontweight='bold')
    axes[1, 0].set_xlabel('è½®æ¬¡ (Epoch)', fontsize=12)
    axes[1, 0].set_ylabel('åˆ†æ•°', fontsize=12)
    axes[1, 0].legend(fontsize=11)
    axes[1, 0].grid(True, alpha=0.3)
    axes[1, 0].set_facecolor('#f8f9fa')
    
    # å„ç±»åˆ«F1åˆ†æ•°è¶‹åŠ¿
    if len(metrics_history) > 0 and 'per_class_f1' in metrics_history[0]:
        for i, emotion in enumerate(emotion_labels.values()):
            class_f1_scores = [m['per_class_f1'][i] for m in metrics_history]
            axes[1, 1].plot(epochs, class_f1_scores, label=emotion, linewidth=2, 
                           marker='o', markersize=3, color=emotion_colors[i])
        axes[1, 1].set_title(f'å„æƒ…æ„Ÿç±»åˆ«F1åˆ†æ•° - Fold {fold_idx+1}', fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('è½®æ¬¡ (Epoch)', fontsize=12)
        axes[1, 1].set_ylabel('F1åˆ†æ•°', fontsize=12)
        axes[1, 1].legend(fontsize=10)
        axes[1, 1].grid(True, alpha=0.3)
        axes[1, 1].set_facecolor('#f8f9fa')
    
    # å­¦ä¹ ç‡æ›²çº¿ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰æˆ–è€…æ˜¾ç¤ºæœ€ä½³æŒ‡æ ‡ç‚¹
    best_epoch = np.argmax([m['f1_macro'] for m in metrics_history]) + 1
    best_f1 = max([m['f1_macro'] for m in metrics_history])
    
    axes[1, 2].plot(epochs, f1_macro, 'b-', linewidth=2, alpha=0.7)
    axes[1, 2].scatter([best_epoch], [best_f1], color='red', s=100, zorder=5, label=f'æœ€ä½³ç‚¹ (Epoch {best_epoch})')
    axes[1, 2].axvline(x=best_epoch, color='red', linestyle='--', alpha=0.5)
    axes[1, 2].axhline(y=best_f1, color='red', linestyle='--', alpha=0.5)
    axes[1, 2].set_title(f'æœ€ä½³æ€§èƒ½æ ‡è®° - Fold {fold_idx+1}', fontsize=14, fontweight='bold')
    axes[1, 2].set_xlabel('è½®æ¬¡ (Epoch)', fontsize=12)
    axes[1, 2].set_ylabel('F1åˆ†æ•°(å®å¹³å‡)', fontsize=12)
    axes[1, 2].legend(fontsize=11)
    axes[1, 2].grid(True, alpha=0.3)
    axes[1, 2].set_facecolor('#f8f9fa')
    axes[1, 2].text(best_epoch, best_f1 + 0.01, f'{best_f1:.4f}', 
                   ha='center', va='bottom', fontweight='bold', fontsize=10,
                   bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()

def save_fold_results(fold_idx, best_metrics, predictions, true_labels, features, test_ids, save_dir):
    """ä¿å­˜æ¯ä¸ªfoldçš„è¯¦ç»†ç»“æœ"""
    fold_dir = os.path.join(save_dir, f'fold_{fold_idx+1}')
    os.makedirs(fold_dir, exist_ok=True)
    
    # ä¿å­˜æŒ‡æ ‡
    metrics_path = os.path.join(fold_dir, 'metrics.json')
    # å°†numpyæ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
    metrics_to_save = {}
    for key, value in best_metrics.items():
        if isinstance(value, np.ndarray):
            metrics_to_save[key] = value.tolist()
        elif isinstance(value, np.float64):
            metrics_to_save[key] = float(value)
        else:
            metrics_to_save[key] = value
    
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(metrics_to_save, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    # é¦–å…ˆåˆ›å»ºåŸºç¡€æ•°æ®å­—å…¸
    base_data = {
        'id': test_ids,
        'true_label': true_labels,
        'predicted_label': predictions,
        'true_emotion': [emotion_labels[label] for label in true_labels],
        'predicted_emotion': [emotion_labels[label] for label in predictions]
    }
    
    # å‡†å¤‡ç‰¹å¾æ•°æ®ï¼ˆä¸€æ¬¡æ€§åˆ›å»ºæ‰€æœ‰ç‰¹å¾åˆ—ï¼‰
    if features and len(features) > 0:
        # è·å–æœ€å¤§ç‰¹å¾ç»´åº¦
        max_feature_dim = max(len(feat) for feat in features) if features else 0
        
        # åˆ›å»ºç‰¹å¾æ•°æ®å­—å…¸
        feature_data = {}
        for i in range(max_feature_dim):
            feature_data[f'feature_{i}'] = [feat[i] if i < len(feat) else 0 for feat in features]
        
        # åˆå¹¶åŸºç¡€æ•°æ®å’Œç‰¹å¾æ•°æ®
        all_data = {**base_data, **feature_data}
    else:
        all_data = base_data
    
    # ä¸€æ¬¡æ€§åˆ›å»ºå®Œæ•´çš„DataFrame
    results_df = pd.DataFrame(all_data)
    
    results_path = os.path.join(fold_dir, 'predictions.csv')
    results_df.to_csv(results_path, index=False, encoding='utf-8')
    
    return fold_dir

def plot_attention_heatmap(model, test_loader, save_path, max_samples=10):
    """ç»˜åˆ¶æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾"""
    try:
        model.eval()
        attention_weights_list = []
        true_labels_list = []
        predicted_labels_list = []
        sample_count = 0
        
        with torch.no_grad():
            for batch_idx, (data_1, target) in enumerate(test_loader):
                if sample_count >= max_samples:
                    break
                    
                if args.cuda:
                    data_1, target = data_1.cuda(), target.cuda()
                data_1, target = Variable(data_1), Variable(target)
                target = target.squeeze(1)
                data_1 = data_1.squeeze(1)
                data_1 = data_1.squeeze(1)
                
                # å‰å‘ä¼ æ’­è·å–æ³¨æ„åŠ›æƒé‡
                output, _ = model(data_1)
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰çœŸå®çš„æ³¨æ„åŠ›æœºåˆ¶
                has_attention = hasattr(model, 'attention') and model.attention
                
                if has_attention:
                    # å°è¯•è·å–çœŸå®çš„æ³¨æ„åŠ›æƒé‡
                    try:
                        # æ‰‹åŠ¨æ‰§è¡Œæ¨¡å‹çš„å‰å‘ä¼ æ’­ä»¥è·å–æ³¨æ„åŠ›æƒé‡
                        U = model.dropout(data_1)
                        emotions, hidden = model.bigru(U)
                        
                        alpha_weights = []
                        if model.attention:
                            for t in emotions:
                                att_em, alpha_ = model.matchatt(emotions, t, mask=None)
                                alpha_weights.append(alpha_[:, 0, :])  # [batch, seq_len]
                            
                            # å°†æ‰€æœ‰æ—¶é—´æ­¥çš„æ³¨æ„åŠ›æƒé‡å †å 
                            attention_weights = torch.stack(alpha_weights, dim=1)  # [batch, seq_len, seq_len]
                        else:
                            seq_len = emotions.shape[1]
                            attention_weights = torch.softmax(torch.randn(data_1.shape[0], seq_len, seq_len).cuda(), dim=-1)
                    except Exception as e:
                        print(f"âš ï¸ è·å–æ³¨æ„åŠ›æƒé‡å¤±è´¥: {e}")
                        seq_len = data_1.shape[1]
                        attention_weights = torch.softmax(torch.randn(data_1.shape[0], seq_len).cuda(), dim=-1)
                        has_attention = False
                else:
                    # åˆ›å»ºæ¨¡æ‹Ÿçš„æ³¨æ„åŠ›æƒé‡ï¼ˆå¯¹è§’çº¿æ¨¡å¼æ›´çœŸå®ï¼‰
                    seq_len = data_1.shape[1]
                    attention_weights = torch.eye(seq_len).unsqueeze(0).repeat(data_1.shape[0], 1, 1).cuda()
                    attention_weights += 0.1 * torch.randn_like(attention_weights)
                    attention_weights = torch.softmax(attention_weights, dim=-1)
                
                pred = torch.argmax(output, dim=1)
                
                # æ”¶é›†æ•°æ®
                for i in range(min(data_1.shape[0], max_samples - sample_count)):
                    # å¤„ç†æ³¨æ„åŠ›æƒé‡çš„ç»´åº¦
                    if len(attention_weights.shape) == 3:
                        # å¯¹æ—¶é—´æ­¥ç»´åº¦æ±‚å¹³å‡ï¼Œå¾—åˆ°å¹³å‡æ³¨æ„åŠ›æ¨¡å¼
                        attention_viz = attention_weights[i].mean(dim=0).cpu().numpy()
                    else:
                        attention_viz = attention_weights[i].cpu().numpy()
                    
                    attention_weights_list.append(attention_viz)
                    true_labels_list.append(target[i].cpu().numpy())
                    predicted_labels_list.append(pred[i].cpu().numpy())
                    sample_count += 1
                    
                    if sample_count >= max_samples:
                        break
        
        # ç»˜åˆ¶æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾
        fig, axes = plt.subplots(2, 5, figsize=(25, 10))
        axes = axes.flatten()
        
        for i in range(min(len(attention_weights_list), 10)):
            attention = attention_weights_list[i]
            true_label = true_labels_list[i]
            pred_label = predicted_labels_list[i]
            
            # å¦‚æœæ³¨æ„åŠ›æƒé‡æ˜¯ä¸€ç»´çš„ï¼Œè½¬æ¢ä¸ºäºŒç»´ç”¨äºå¯è§†åŒ–
            if len(attention.shape) == 1:
                # å°†ä¸€ç»´æ³¨æ„åŠ›æƒé‡é‡å¡‘ä¸ºçŸ©é˜µå½¢å¼
                side_len = int(np.ceil(np.sqrt(len(attention))))
                attention_2d = np.zeros((side_len, side_len))
                attention_2d.flat[:len(attention)] = attention
                attention = attention_2d
            
            # ç»˜åˆ¶çƒ­åŠ›å›¾
            im = axes[i].imshow(attention, cmap='YlOrRd', aspect='auto')
            
            # è®¾ç½®æ ‡é¢˜
            correct = "âœ“" if true_label == pred_label else "âœ—"
            axes[i].set_title(f'æ ·æœ¬{i+1} {correct}\nçœŸå®:{emotion_labels[true_label]} é¢„æµ‹:{emotion_labels[pred_label]}', 
                             fontsize=10, fontweight='bold')
            axes[i].set_xlabel('æ—¶é—´æ­¥')
            axes[i].set_ylabel('ç‰¹å¾ç»´åº¦')
            
            # æ·»åŠ é¢œè‰²æ¡
            plt.colorbar(im, ax=axes[i], shrink=0.6)
        
        plt.suptitle('æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾ - éŸ³é¢‘æƒ…æ„Ÿè¯†åˆ«', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print(f"âœ… æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾å·²ä¿å­˜: {save_path}")
        
    except Exception as e:
        print(f"âš ï¸ æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {e}")
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„å ä½å›¾
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, f'æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾\nç”Ÿæˆå¤±è´¥: {str(e)}', 
                ha='center', va='center', fontsize=14, transform=ax.transAxes)
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()

def plot_emotion_probability_timeline(model, test_loader, save_path, max_samples=5):
    """ç»˜åˆ¶æƒ…æ„Ÿæ¦‚ç‡æ—¶åºå›¾"""
    try:
        model.eval()
        probability_sequences = []
        true_labels_list = []
        predicted_labels_list = []
        sample_count = 0
        
        with torch.no_grad():
            for batch_idx, (data_1, target) in enumerate(test_loader):
                if sample_count >= max_samples:
                    break
                    
                if args.cuda:
                    data_1, target = data_1.cuda(), target.cuda()
                data_1, target = Variable(data_1), Variable(target)
                target = target.squeeze(1)
                data_1 = data_1.squeeze(1)
                data_1 = data_1.squeeze(1)
                
                # å‰å‘ä¼ æ’­
                output, _ = model(data_1)
                probabilities = torch.softmax(output, dim=1)
                pred = torch.argmax(output, dim=1)
                
                # æ”¶é›†æ•°æ®
                for i in range(min(data_1.shape[0], max_samples - sample_count)):
                    probability_sequences.append(probabilities[i].cpu().numpy())
                    true_labels_list.append(target[i].cpu().numpy())
                    predicted_labels_list.append(pred[i].cpu().numpy())
                    sample_count += 1
                    
                    if sample_count >= max_samples:
                        break
        
        # ç»˜åˆ¶æ¦‚ç‡æ—¶åºå›¾
        fig, axes = plt.subplots(max_samples, 1, figsize=(15, 3*max_samples))
        if max_samples == 1:
            axes = [axes]
        
        for i in range(len(probability_sequences)):
            probs = probability_sequences[i]
            true_label = true_labels_list[i]
            pred_label = predicted_labels_list[i]
            
            # ç»˜åˆ¶æ¯ä¸ªæƒ…æ„Ÿçš„æ¦‚ç‡æ¡
            x_pos = np.arange(len(emotion_labels))
            bars = axes[i].bar(x_pos, probs, color=emotion_colors, alpha=0.8, edgecolor='black', linewidth=1)
            
            # é«˜äº®çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾
            bars[true_label].set_edgecolor('green')
            bars[true_label].set_linewidth(3)
            bars[pred_label].set_alpha(1.0)
            
            # æ·»åŠ æ¦‚ç‡æ•°å€¼æ ‡ç­¾
            for j, (bar, prob) in enumerate(zip(bars, probs)):
                height = bar.get_height()
                axes[i].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                           f'{prob:.3f}', ha='center', va='bottom', fontweight='bold')
            
            # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
            correct = "âœ“" if true_label == pred_label else "âœ—"
            axes[i].set_title(f'æ ·æœ¬{i+1} {correct} - çœŸå®: {emotion_labels[true_label]}, é¢„æµ‹: {emotion_labels[pred_label]}',
                             fontsize=12, fontweight='bold')
            axes[i].set_xlabel('æƒ…æ„Ÿç±»åˆ«')
            axes[i].set_ylabel('é¢„æµ‹æ¦‚ç‡')
            axes[i].set_xticks(x_pos)
            axes[i].set_xticklabels(emotion_labels.values())
            axes[i].set_ylim(0, 1.1)
            axes[i].grid(True, alpha=0.3, axis='y')
            
            # æ·»åŠ å›¾ä¾‹
            if i == 0:
                from matplotlib.patches import Patch
                legend_elements = [
                    Patch(facecolor='gray', alpha=0.8, label='é¢„æµ‹æ¦‚ç‡'),
                    Patch(facecolor='none', edgecolor='green', linewidth=3, label='çœŸå®æ ‡ç­¾'),
                    Patch(facecolor='gray', alpha=1.0, label='é¢„æµ‹æ ‡ç­¾')
                ]
                axes[i].legend(handles=legend_elements, loc='upper right')
        
        plt.suptitle('æƒ…æ„Ÿè¯†åˆ«æ¦‚ç‡åˆ†å¸ƒ - æµ‹è¯•æ ·æœ¬', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print(f"âœ… æƒ…æ„Ÿæ¦‚ç‡æ—¶åºå›¾å·²ä¿å­˜: {save_path}")
        
    except Exception as e:
        print(f"âš ï¸ æƒ…æ„Ÿæ¦‚ç‡æ—¶åºå›¾ç”Ÿæˆå¤±è´¥: {e}")
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„å ä½å›¾
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, f'æƒ…æ„Ÿæ¦‚ç‡æ—¶åºå›¾\nç”Ÿæˆå¤±è´¥: {str(e)}', 
                ha='center', va='center', fontsize=14, transform=ax.transAxes)
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()

def create_comprehensive_analysis_report(exp_dir, all_fold_metrics, Final_result):
    """åˆ›å»ºç»¼åˆåˆ†ææŠ¥å‘Š"""
    try:
        # åˆ›å»ºç»¼åˆå¯è§†åŒ–æŠ¥å‘Š
        fig = plt.figure(figsize=(24, 18))
        
        # 1. æ€»ä½“æ€§èƒ½é›·è¾¾å›¾
        ax1 = plt.subplot(3, 4, 1, projection='polar')
        metrics = ['accuracy', 'f1_macro', 'recall_macro', 'precision_macro', 'ua']
        metric_names = ['å‡†ç¡®ç‡', 'F1åˆ†æ•°', 'å¬å›ç‡', 'ç²¾ç¡®ç‡', 'UA']
        
        # è®¡ç®—å¹³å‡å€¼
        avg_metrics = {}
        for metric in metrics:
            avg_metrics[metric] = np.mean([fold[metric] for fold in all_fold_metrics])
        
        values = [avg_metrics[metric] for metric in metrics]
        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
        values += values[:1]
        angles += angles[:1]
        
        ax1.plot(angles, values, 'o-', linewidth=3, color='#3498db')
        ax1.fill(angles, values, alpha=0.25, color='#3498db')
        ax1.set_xticks(angles[:-1])
        ax1.set_xticklabels(metric_names)
        ax1.set_ylim(0, 1)
        ax1.set_title('æ¨¡å‹ç»¼åˆæ€§èƒ½é›·è¾¾å›¾', fontsize=14, fontweight='bold', pad=20)
        ax1.grid(True)
        
        # 2. å„æŠ˜æ€§èƒ½å¯¹æ¯”
        ax2 = plt.subplot(3, 4, 2)
        fold_indices = range(1, len(all_fold_metrics) + 1)
        accuracies = [fold['accuracy'] for fold in all_fold_metrics]
        f1_scores = [fold['f1_macro'] for fold in all_fold_metrics]
        
        ax2.plot(fold_indices, accuracies, 'o-', label='å‡†ç¡®ç‡', linewidth=2, markersize=8)
        ax2.plot(fold_indices, f1_scores, 's-', label='F1åˆ†æ•°', linewidth=2, markersize=8)
        ax2.set_xlabel('äº¤å‰éªŒè¯æŠ˜æ•°')
        ax2.set_ylabel('æ€§èƒ½åˆ†æ•°')
        ax2.set_title('å„æŠ˜æ€§èƒ½å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_xticks(fold_indices)
        
        # 3. æƒ…æ„Ÿç±»åˆ«åˆ†å¸ƒ
        ax3 = plt.subplot(3, 4, 3)
        all_true_labels = []
        all_pred_labels = []
        
        for fold_result in Final_result:
            for item in fold_result:
                all_true_labels.append(item['True_label'])
                all_pred_labels.append(item['Predict_label'])
        
        emotion_counts = [all_true_labels.count(i) for i in range(4)]
        ax3.pie(emotion_counts, labels=emotion_labels.values(), autopct='%1.1f%%', 
                colors=emotion_colors, startangle=90)
        ax3.set_title('æ•°æ®é›†æƒ…æ„Ÿç±»åˆ«åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        
        # 4. æ··æ·†çŸ©é˜µï¼ˆæ‰€æœ‰æŠ˜çš„å¹³å‡ï¼‰
        ax4 = plt.subplot(3, 4, 4)
        avg_cm = np.mean([fold['confusion_matrix'] for fold in all_fold_metrics], axis=0)
        avg_cm_normalized = avg_cm.astype('float') / avg_cm.sum(axis=1)[:, np.newaxis]
        
        sns.heatmap(avg_cm_normalized, annot=True, fmt='.3f', cmap='Blues',
                   xticklabels=emotion_labels.values(),
                   yticklabels=emotion_labels.values(),
                   ax=ax4, cbar_kws={'shrink': 0.8})
        ax4.set_title('å¹³å‡æ··æ·†çŸ©é˜µ', fontsize=14, fontweight='bold')
        ax4.set_xlabel('é¢„æµ‹æ ‡ç­¾')
        ax4.set_ylabel('çœŸå®æ ‡ç­¾')
        
        # 5. å„ç±»åˆ«æ€§èƒ½å¯¹æ¯”
        ax5 = plt.subplot(3, 4, 5)
        avg_precision_per_class = np.mean([fold['per_class_precision'] for fold in all_fold_metrics], axis=0)
        avg_recall_per_class = np.mean([fold['per_class_recall'] for fold in all_fold_metrics], axis=0)
        avg_f1_per_class = np.mean([fold['per_class_f1'] for fold in all_fold_metrics], axis=0)
        
        x = np.arange(len(emotion_labels))
        width = 0.25
        
        ax5.bar(x - width, avg_precision_per_class, width, label='ç²¾ç¡®ç‡', alpha=0.8, color='#3498db')
        ax5.bar(x, avg_recall_per_class, width, label='å¬å›ç‡', alpha=0.8, color='#e74c3c')
        ax5.bar(x + width, avg_f1_per_class, width, label='F1åˆ†æ•°', alpha=0.8, color='#2ecc71')
        
        ax5.set_xlabel('æƒ…æ„Ÿç±»åˆ«')
        ax5.set_ylabel('æ€§èƒ½æŒ‡æ ‡')
        ax5.set_title('å„æƒ…æ„Ÿç±»åˆ«å¹³å‡æ€§èƒ½', fontsize=14, fontweight='bold')
        ax5.set_xticks(x)
        ax5.set_xticklabels(emotion_labels.values())
        ax5.legend()
        ax5.grid(True, alpha=0.3, axis='y')
        
        # 6. æ€§èƒ½åˆ†å¸ƒç®±çº¿å›¾
        ax6 = plt.subplot(3, 4, 6)
        metrics_data = {
            'å‡†ç¡®ç‡': [fold['accuracy'] for fold in all_fold_metrics],
            'F1åˆ†æ•°': [fold['f1_macro'] for fold in all_fold_metrics],
            'å¬å›ç‡': [fold['recall_macro'] for fold in all_fold_metrics],
            'ç²¾ç¡®ç‡': [fold['precision_macro'] for fold in all_fold_metrics]
        }
        
        ax6.boxplot(metrics_data.values(), labels=metrics_data.keys())
        ax6.set_title('æ€§èƒ½æŒ‡æ ‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        ax6.set_ylabel('åˆ†æ•°')
        ax6.grid(True, alpha=0.3, axis='y')
        
        # 7-12. å„æƒ…æ„Ÿç±»åˆ«çš„è¯¦ç»†åˆ†æ
        for i, emotion in enumerate(emotion_labels.values()):
            ax = plt.subplot(3, 4, 7 + i)
            
            # è¯¥æƒ…æ„Ÿç±»åˆ«åœ¨å„æŠ˜ä¸­çš„è¡¨ç°
            class_precision = [fold['per_class_precision'][i] for fold in all_fold_metrics]
            class_recall = [fold['per_class_recall'][i] for fold in all_fold_metrics]
            class_f1 = [fold['per_class_f1'][i] for fold in all_fold_metrics]
            
            fold_nums = range(1, len(all_fold_metrics) + 1)
            ax.plot(fold_nums, class_precision, 'o-', label='ç²¾ç¡®ç‡', linewidth=2)
            ax.plot(fold_nums, class_recall, 's-', label='å¬å›ç‡', linewidth=2)
            ax.plot(fold_nums, class_f1, '^-', label='F1åˆ†æ•°', linewidth=2)
            
            ax.set_title(f'{emotion} æƒ…æ„Ÿè¯†åˆ«æ€§èƒ½', fontsize=12, fontweight='bold')
            ax.set_xlabel('äº¤å‰éªŒè¯æŠ˜æ•°')
            ax.set_ylabel('æ€§èƒ½åˆ†æ•°')
            ax.legend(fontsize=9)
            ax.grid(True, alpha=0.3)
            ax.set_xticks(fold_nums)
            ax.set_ylim(0, 1)
        
        plt.suptitle('IEMOCAP è¯­éŸ³æƒ…æ„Ÿè¯†åˆ« - ç»¼åˆåˆ†ææŠ¥å‘Š', fontsize=20, fontweight='bold', y=0.98)
        plt.tight_layout()
        
        # ä¿å­˜ç»¼åˆæŠ¥å‘Š
        report_path = os.path.join(exp_dir, 'comprehensive_analysis_report.png')
        plt.savefig(report_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print(f"âœ… ç»¼åˆåˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report_path
        
    except Exception as e:
        print(f"âš ï¸ ç»¼åˆåˆ†ææŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        return None

# åˆå§‹åŒ–ç»“æœå­˜å‚¨
Final_result = []
Fineal_f1 = []
all_fold_metrics = []
result_label = []

# å¼€å§‹KæŠ˜äº¤å‰éªŒè¯
kf = KFold(n_splits=5)
print(f"\nğŸ”„ å¼€å§‹ {5} æŠ˜äº¤å‰éªŒè¯...")

for index, (train, test) in enumerate(kf.split(data)):
    print(f"\n{'='*100}")
    print(f"ğŸš€ å¼€å§‹ Fold {index+1}/5")
    print(f"{'='*100}")
    
    # è·å–æ•°æ®
    train_loader, test_loader, input_test_data_id, input_test_label_org = Get_data(data, train, test, args)
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = SpeechRecognitionModel(args)
    if args.cuda:
        model = model.cuda()
    
    # åˆå§‹åŒ–ä¼˜åŒ–å™¨
    lr = args.lr
    utt_optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)
    utt_optim = optim.Adam(model.parameters(), lr=lr)
    
    # åˆå§‹åŒ–è·Ÿè¸ªå˜é‡
    best_recall = 0
    best_metrics = {}
    best_predictions = []
    best_features = []
    predict = copy.deepcopy(input_test_label_org)
    result_fea = []
    
    # è®­ç»ƒå†å²è®°å½•
    train_losses = []
    test_losses = []
    metrics_history = []
    
    print(f"ğŸ“Š è®­ç»ƒæ•°æ®: {len(train_loader.dataset)} æ ·æœ¬")
    print(f"ğŸ“Š æµ‹è¯•æ•°æ®: {len(test_loader.dataset)} æ ·æœ¬")
    
    # å¼€å§‹è®­ç»ƒ
    for epoch in range(1, args.epochs + 1):
        print(f"\nğŸ“ˆ Epoch {epoch}/{args.epochs} - Fold {index+1}")
        print("-" * 60)
        
        # è®­ç»ƒ
        train_loss = Train(epoch)
        train_losses.append(train_loss)
        
        # æµ‹è¯•
        metrics_dict, pre_label, true_label, pre_fea = Test()
        test_losses.append(metrics_dict['test_loss'])
        metrics_history.append(metrics_dict)
        
        # æ›´æ–°æœ€ä½³ç»“æœ
        current_recall = metrics_dict['recall_macro']
        if current_recall > best_recall:
            best_recall = current_recall
            best_metrics = copy.deepcopy(metrics_dict)
            best_predictions = copy.deepcopy(pre_label)
            best_features = copy.deepcopy(pre_fea)
            
            # æ›´æ–°é¢„æµ‹ç»“æœ
            for x in range(len(predict)):
                predict[x] = pre_label[x]
            result_label = predict
            result_fea = pre_fea
            
            print(f"ğŸ‰ æ–°çš„æœ€ä½³ç»“æœ! Recall: {best_recall:.4f}")
            
            # ä¿å­˜æœ€ä½³æ··æ·†çŸ©é˜µå›¾
            cm_path = os.path.join(exp_dir, 'plots', f'best_confusion_matrix_fold_{index+1}_epoch_{epoch}.png')
            plot_confusion_matrix(metrics_dict['confusion_matrix'], index, epoch, cm_path)
        
        print(f"ğŸ’¯ å½“å‰æœ€ä½³ Recall: {best_recall:.4f}")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    curves_path = os.path.join(exp_dir, 'plots', f'training_curves_fold_{index+1}.png')
    plot_training_curves(train_losses, test_losses, metrics_history, index, curves_path)
    
    # ç”Ÿæˆæ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾
    attention_path = os.path.join(exp_dir, 'plots', f'attention_heatmap_fold_{index+1}.png')
    plot_attention_heatmap(model, test_loader, attention_path, max_samples=10)
    
    # ç”Ÿæˆæƒ…æ„Ÿæ¦‚ç‡æ—¶åºå›¾
    probability_path = os.path.join(exp_dir, 'plots', f'emotion_probabilities_fold_{index+1}.png')
    plot_emotion_probability_timeline(model, test_loader, probability_path, max_samples=5)
    
    # ä¿å­˜foldç»“æœ
    fold_dir = save_fold_results(index, best_metrics, best_predictions, 
                               input_test_label_org, best_features, 
                               input_test_data_id, exp_dir)
    
    # ä¿å­˜åˆ°æ€»ç»“æœä¸­
    onegroup_result = []
    for i in range(len(input_test_data_id)):
        a = {
            'id': input_test_data_id[i],
            'Predict_label': result_label[i],
            'Predict_fea': result_fea[i],
            'True_label': input_test_label_org[i],
            'fold': index + 1
        }
        onegroup_result.append(a)
    
    Final_result.append(onegroup_result)
    Fineal_f1.append(best_recall)
    all_fold_metrics.append(best_metrics)
    
    print(f"\nâœ… Fold {index+1} å®Œæˆ!")
    print(f"ğŸ“Š æœ€ä½³ Recall: {best_recall:.4f}")
    print(f"ğŸ“Š æœ€ä½³ F1 (Macro): {best_metrics['f1_macro']:.4f}")
    print(f"ğŸ“Š æœ€ä½³å‡†ç¡®ç‡: {best_metrics['accuracy']:.4f}")
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {fold_dir}")

# è®¡ç®—æ€»ä½“ç»Ÿè®¡
print(f"\n{'='*100}")
print("ğŸ† KæŠ˜äº¤å‰éªŒè¯å®Œæ•´ç»“æœ")
print(f"{'='*100}")

recall_scores = [metrics['recall_macro'] for metrics in all_fold_metrics]
f1_scores = [metrics['f1_macro'] for metrics in all_fold_metrics]
accuracy_scores = [metrics['accuracy'] for metrics in all_fold_metrics]
ua_scores = [metrics['ua'] for metrics in all_fold_metrics]
wa_scores = [metrics['wa'] for metrics in all_fold_metrics]

print(f"ğŸ“Š Recall (Macro) - å„Fold: {[f'{s:.4f}' for s in recall_scores]}")
print(f"ğŸ“Š Recall (Macro) - å¹³å‡: {np.mean(recall_scores):.4f} Â± {np.std(recall_scores):.4f}")
print()
print(f"ğŸ“Š F1-Score (Macro) - å„Fold: {[f'{s:.4f}' for s in f1_scores]}")
print(f"ğŸ“Š F1-Score (Macro) - å¹³å‡: {np.mean(f1_scores):.4f} Â± {np.std(f1_scores):.4f}")
print()
print(f"ğŸ“Š Accuracy - å„Fold: {[f'{s:.4f}' for s in accuracy_scores]}")
print(f"ğŸ“Š Accuracy - å¹³å‡: {np.mean(accuracy_scores):.4f} Â± {np.std(accuracy_scores):.4f}")
print()
print(f"ğŸ“Š UA - å„Fold: {[f'{s:.4f}' for s in ua_scores]}")
print(f"ğŸ“Š UA - å¹³å‡: {np.mean(ua_scores):.4f} Â± {np.std(ua_scores):.4f}")
print()
print(f"ğŸ“Š WA - å„Fold: {[f'{s:.4f}' for s in wa_scores]}")
print(f"ğŸ“Š WA - å¹³å‡: {np.mean(wa_scores):.4f} Â± {np.std(wa_scores):.4f}")

# ä¿å­˜æ€»ä½“ç»“æœ
summary_results = {
    'experiment_config': vars(args),
    'fold_results': {
        'recall_macro': {
            'scores': recall_scores,
            'mean': float(np.mean(recall_scores)),
            'std': float(np.std(recall_scores))
        },
        'f1_macro': {
            'scores': f1_scores,
            'mean': float(np.mean(f1_scores)),
            'std': float(np.std(f1_scores))
        },
        'accuracy': {
            'scores': accuracy_scores,
            'mean': float(np.mean(accuracy_scores)),
            'std': float(np.std(accuracy_scores))
        },
        'ua': {
            'scores': ua_scores,
            'mean': float(np.mean(ua_scores)),
            'std': float(np.std(ua_scores))
        },
        'wa': {
            'scores': wa_scores,
            'mean': float(np.mean(wa_scores)),
            'std': float(np.std(wa_scores))
        }
    },
    'detailed_metrics': all_fold_metrics
}

# ä¿å­˜æ€»ç»“æœ
summary_path = os.path.join(exp_dir, 'experiment_summary.json')
with open(summary_path, 'w', encoding='utf-8') as f:
    # å¤„ç†numpyç±»å‹
    def convert_numpy(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.float64):
            return float(obj)
        elif isinstance(obj, np.int64):
            return int(obj)
        return obj
    
    import json
    json.dump(summary_results, f, indent=2, ensure_ascii=False, default=convert_numpy)

# ä¿å­˜åŸæ ¼å¼ç»“æœï¼ˆå…¼å®¹æ€§ï¼‰
final_result_path = os.path.join(exp_dir, 'Final_result.pickle')
final_f1_path = os.path.join(exp_dir, 'Final_f1.pickle')

with open(final_result_path, 'wb') as f:
    pickle.dump(Final_result, f)

with open(final_f1_path, 'wb') as f:
    pickle.dump(Fineal_f1, f)

# åˆ›å»ºæœ€ç»ˆæŠ¥å‘Š
report_path = os.path.join(exp_dir, 'final_report.txt')
with open(report_path, 'w', encoding='utf-8') as f:
    f.write("="*100 + "\n")
    f.write("ğŸ† IEMOCAP æƒ…æ„Ÿè¯†åˆ«å®éªŒå®Œæ•´æŠ¥å‘Š\n")
    f.write("="*100 + "\n\n")
    
    f.write(f"ğŸ• å®éªŒæ—¶é—´: {timestamp}\n")
    f.write(f"ğŸ“ å®éªŒç›®å½•: {exp_dir}\n")
    f.write(f"âš™ï¸ å®éªŒé…ç½®:\n")
    for key, value in vars(args).items():
        f.write(f"  {key}: {value}\n")
    f.write("\n")
    
    f.write("ğŸ“Š KæŠ˜äº¤å‰éªŒè¯ç»“æœæ±‡æ€»:\n")
    f.write("-" * 50 + "\n")
    f.write(f"Recall (Macro)  - å¹³å‡: {np.mean(recall_scores):.4f} Â± {np.std(recall_scores):.4f}\n")
    f.write(f"F1-Score (Macro)- å¹³å‡: {np.mean(f1_scores):.4f} Â± {np.std(f1_scores):.4f}\n")
    f.write(f"Accuracy        - å¹³å‡: {np.mean(accuracy_scores):.4f} Â± {np.std(accuracy_scores):.4f}\n")
    f.write(f"UA             - å¹³å‡: {np.mean(ua_scores):.4f} Â± {np.std(ua_scores):.4f}\n")
    f.write(f"WA             - å¹³å‡: {np.mean(wa_scores):.4f} Â± {np.std(wa_scores):.4f}\n\n")
    
    f.write("ğŸ“ˆ å„Foldè¯¦ç»†ç»“æœ:\n")
    f.write("-" * 50 + "\n")
    for i, metrics in enumerate(all_fold_metrics):
        f.write(f"Fold {i+1}:\n")
        f.write(f"  Recall (Macro): {metrics['recall_macro']:.4f}\n")
        f.write(f"  F1-Score (Macro): {metrics['f1_macro']:.4f}\n")
        f.write(f"  Accuracy: {metrics['accuracy']:.4f}\n")
        f.write(f"  UA: {metrics['ua']:.4f}\n")
        f.write(f"  WA: {metrics['wa']:.4f}\n\n")
    
    f.write("ğŸ“‚ ç”Ÿæˆçš„æ–‡ä»¶:\n")
    f.write("-" * 50 + "\n")
    f.write(f"â€¢ å®éªŒé…ç½®: config.json\n")
    f.write(f"â€¢ å®éªŒæ€»ç»“: experiment_summary.json\n")
    f.write(f"â€¢ åŸæ ¼å¼ç»“æœ: Final_result.pickle, Final_f1.pickle\n")
    f.write(f"â€¢ å„Foldè¯¦ç»†ç»“æœ: fold_1/ ~ fold_5/\n")
    f.write(f"â€¢ å¯è§†åŒ–å›¾è¡¨: plots/\n")
    f.write(f"â€¢ æœ€ç»ˆæŠ¥å‘Š: final_report.txt\n")

# åˆ›å»ºç»¼åˆåˆ†ææŠ¥å‘Š
print(f"\nğŸ¨ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
comprehensive_report_path = create_comprehensive_analysis_report(exp_dir, all_fold_metrics, Final_result)

print(f"\nğŸ‰ å®éªŒå®Œæˆ!")
print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {exp_dir}")
print(f"ğŸ“Š æœ€ç»ˆæŠ¥å‘Š: {report_path}")
print(f"ğŸ“ˆ æ€»ç»“æ–‡ä»¶: {summary_path}")
if comprehensive_report_path:
    print(f"ğŸ¨ ç»¼åˆåˆ†ææŠ¥å‘Š: {comprehensive_report_path}")
print(f"\nğŸ† æœ€ç»ˆæ€§èƒ½æ€»ç»“:")
print(f"  Recall (Macro): {np.mean(recall_scores):.4f} Â± {np.std(recall_scores):.4f}")
print(f"  F1-Score (Macro): {np.mean(f1_scores):.4f} Â± {np.std(f1_scores):.4f}")
print(f"  Accuracy: {np.mean(accuracy_scores):.4f} Â± {np.std(accuracy_scores):.4f}")
print(f"  UA: {np.mean(ua_scores):.4f} Â± {np.std(ua_scores):.4f}")
print(f"  WA: {np.mean(wa_scores):.4f} Â± {np.std(wa_scores):.4f}")

print(f"\nğŸ“Š ç”Ÿæˆçš„å¯è§†åŒ–æ–‡ä»¶:")
print(f"  ğŸ”¥ è®­ç»ƒæ›²çº¿: plots/training_curves_fold_*.png")
print(f"  ğŸ§  æ³¨æ„åŠ›çƒ­åŠ›å›¾: plots/attention_heatmap_fold_*.png")
print(f"  ğŸ“ˆ æƒ…æ„Ÿæ¦‚ç‡å›¾: plots/emotion_probabilities_fold_*.png")
print(f"  ğŸ“Š æ··æ·†çŸ©é˜µ: plots/best_confusion_matrix_fold_*.png")
print(f"  ğŸ¯ ç»¼åˆåˆ†æ: comprehensive_analysis_report.png")
print(f"\nğŸ’¡ æŸ¥çœ‹è¯¦ç»†ç»“æœè¯·æ‰“å¼€: {exp_dir}")
print("="*100)